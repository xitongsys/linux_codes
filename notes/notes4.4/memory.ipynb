{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memory zone\n",
    "\n",
    "![zone01](resources/zone01.png)[ref](https://stackoverflow.com/questions/18061218/how-linux-kernel-decide-to-which-memory-zone-to-use)\n",
    "\n",
    "![zone02](resources/zone02.png)\n",
    "![zone03](resources/zone03.png)\n",
    "\n",
    "[x64 mem layout](https://unix.stackexchange.com/questions/509607/how-a-64-bit-process-virtual-address-space-is-divided-in-linux)\n",
    "\n",
    "[canonical address](https://en.wikipedia.org/wiki/X86-64#Virtual_address_space_details)\n",
    "\n",
    "---------------------\n",
    "\n",
    "## linux2.6/include/linux/mmzone.h\n",
    "\n",
    "```c\n",
    "#define ZONE_DMA\t\t0\n",
    "#define ZONE_NORMAL\t\t1\n",
    "#define ZONE_HIGHMEM\t\t2\n",
    "#define MAX_NR_ZONES\t\t3\n",
    "#define GFP_ZONEMASK\t0x03\n",
    "\n",
    "/*\n",
    " * One allocation request operates on a zonelist. A zonelist\n",
    " * is a list of zones, the first one is the 'goal' of the\n",
    " * allocation, the other zones are fallback zones, in decreasing\n",
    " * priority.\n",
    " *\n",
    " * Right now a zonelist takes up less than a cacheline. We never\n",
    " * modify it apart from boot-up, and only a few indices are used,\n",
    " * so despite the zonelist table being relatively big, the cache\n",
    " * footprint of this construct is very small.\n",
    " */\n",
    "struct zonelist {\n",
    "\tstruct zone *zones[MAX_NUMNODES * MAX_NR_ZONES + 1]; // NULL delimited\n",
    "};\n",
    "\n",
    "```\n",
    "1. define some consts\n",
    "\n",
    "```c\n",
    "/*\n",
    " * The pg_data_t structure is used in machines with CONFIG_DISCONTIGMEM\n",
    " * (mostly NUMA machines?) to denote a higher-level memory zone than the\n",
    " * zone denotes.\n",
    " *\n",
    " * On NUMA machines, each NUMA node would have a pg_data_t to describe\n",
    " * it's memory layout.\n",
    " *\n",
    " * Memory statistics and page replacement data structures are maintained on a\n",
    " * per-zone basis.\n",
    " */\n",
    "struct bootmem_data;\n",
    "typedef struct pglist_data {\n",
    "\tstruct zone node_zones[MAX_NR_ZONES];\n",
    "\tstruct zonelist node_zonelists[MAX_NR_ZONES];\n",
    "\tint nr_zones;\n",
    "\tstruct page *node_mem_map;\n",
    "\tunsigned long *valid_addr_bitmap;\n",
    "\tstruct bootmem_data *bdata;\n",
    "\tunsigned long node_start_pfn;\n",
    "\tunsigned long node_present_pages; /* total number of physical pages */\n",
    "\tunsigned long node_spanned_pages; /* total size of physical page\n",
    "\t\t\t\t\t     range, including holes */\n",
    "\tint node_id;\n",
    "\tstruct pglist_data *pgdat_next;\n",
    "\twait_queue_head_t       kswapd_wait;\n",
    "} pg_data_t;\n",
    "\n",
    "#define node_present_pages(nid)\t(NODE_DATA(nid)->node_present_pages)\n",
    "#define node_spanned_pages(nid)\t(NODE_DATA(nid)->node_spanned_pages)\n",
    "\n",
    "```\n",
    "\n",
    "1. NUMA node\n",
    "\n",
    "2. `node_mem_map` array of page descriptors of the node\n",
    "\n",
    "----------------\n",
    "\n",
    "![memzone01](resources/memoryzone01.png)\n",
    "![memzone02](resources/memoryzone02.png)\n",
    "\n",
    "\n",
    "### linux2.6/include/linux/mm.h\n",
    "\n",
    "```c\n",
    "/*\n",
    " * The zone field is never updated after free_area_init_core()\n",
    " * sets it, so none of the operations on it need to be atomic.\n",
    " * We'll have up to (MAX_NUMNODES * MAX_NR_ZONES) zones total,\n",
    " * so we use (MAX_NODES_SHIFT + MAX_ZONES_SHIFT) here to get enough bits.\n",
    " */\n",
    "#define NODEZONE_SHIFT (sizeof(page_flags_t)*8 - MAX_NODES_SHIFT - MAX_ZONES_SHIFT)\n",
    "#define NODEZONE(node, zone)\t((node << ZONES_SHIFT) | zone)\n",
    "\n",
    "static inline unsigned long page_zonenum(struct page *page)\n",
    "{\n",
    "\treturn (page->flags >> NODEZONE_SHIFT) & (~(~0UL << ZONES_SHIFT));\n",
    "}\n",
    "static inline unsigned long page_to_nid(struct page *page)\n",
    "{\n",
    "\treturn (page->flags >> (NODEZONE_SHIFT + ZONES_SHIFT));\n",
    "}\n",
    "\n",
    "struct zone;\n",
    "extern struct zone *zone_table[];\n",
    "\n",
    "static inline struct zone *page_zone(struct page *page)\n",
    "{\n",
    "\treturn zone_table[page->flags >> NODEZONE_SHIFT];\n",
    "}\n",
    "\n",
    "static inline void set_page_zone(struct page *page, unsigned long nodezone_num)\n",
    "{\n",
    "\tpage->flags &= ~(~0UL << NODEZONE_SHIFT);\n",
    "\tpage->flags |= nodezone_num << NODEZONE_SHIFT;\n",
    "}\n",
    "```\n",
    "\n",
    "1. `zone_table` is a global zone array. the `node num` and `zone num` is stored in the high bits of `page->flags`\n",
    "\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linux2.6/include/linux/gfp.h\n",
    "\n",
    "![gfp01](resources/gfp01.png)\n",
    "\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zone allocator\n",
    "\n",
    "![memoryzone03](resources/memoryzone03.png)\n",
    "![memoryzone04](resources/memoryzone04.png)\n",
    "\n",
    "\n",
    "1. linux 最底层都是通过buddy system来管理物理页\n",
    "\n",
    "2. 每个进程都有自己的vma（virtual memory area） list/red-black tree来管理各自的虚拟内存段\n",
    "\n",
    "3. 每个进程通过自己的多层 page table 将虚拟地址和物理地址联系起来\n",
    "\n",
    "4. 无论系统进程还是用户进程，当在实际分配物理内存页的时候，底层都是通过buddy system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## buddy system algorithm\n",
    "\n",
    "![buddy01](resources/buddy01.png)\n",
    "![buddy02](resources/buddy02.png)\n",
    "\n",
    "-----------------\n",
    "\n",
    "## linux2.6/include/linux/mmzone.h\n",
    "\n",
    "```c\n",
    "struct free_area {\n",
    "\tstruct list_head\tfree_list;\n",
    "\tunsigned long\t\t*map;\n",
    "};\n",
    "\n",
    "/*\n",
    "    * free areas of different sizes\n",
    "    */\n",
    "struct free_area\tfree_area[MAX_ORDER];\n",
    "```\n",
    "1. free_area is the buddy list array of different orders.\n",
    "\n",
    "\n",
    "## linux2.6/mm/page_alloc.c\n",
    "\n",
    "### allocate one page\n",
    "\n",
    "```c\n",
    "/* \n",
    " * Do the hard work of removing an element from the buddy allocator.\n",
    " * Call me with the zone->lock already held.\n",
    " */\n",
    "static struct page *__rmqueue(struct zone *zone, unsigned int order)\n",
    "{\n",
    "\tstruct free_area * area;\n",
    "\tunsigned int current_order;\n",
    "\tstruct page *page;\n",
    "\tunsigned int index;\n",
    "\n",
    "\tfor (current_order = order; current_order < MAX_ORDER; ++current_order) {\n",
    "\t\tarea = zone->free_area + current_order;\n",
    "\t\tif (list_empty(&area->free_list))\n",
    "\t\t\tcontinue;\n",
    "\n",
    "\t\tpage = list_entry(area->free_list.next, struct page, list);\n",
    "\t\tlist_del(&page->list);\n",
    "\t\tindex = page - zone->zone_mem_map;\n",
    "\t\tif (current_order != MAX_ORDER-1)\n",
    "\t\t\tMARK_USED(index, current_order, area);\n",
    "\t\tzone->free_pages -= 1UL << order;\n",
    "\t\treturn expand(zone, page, index, order, current_order, area);\n",
    "\t}\n",
    "\n",
    "\treturn NULL;\n",
    "}\n",
    "\n",
    "static inline struct page *\n",
    "expand(struct zone *zone, struct page *page,\n",
    "\t unsigned long index, int low, int high, struct free_area *area)\n",
    "{\n",
    "\tunsigned long size = 1 << high;\n",
    "\n",
    "\twhile (high > low) {\n",
    "\t\tBUG_ON(bad_range(zone, page));\n",
    "\t\tarea--;\n",
    "\t\thigh--;\n",
    "\t\tsize >>= 1;\n",
    "\t\tlist_add(&page->list, &area->free_list);\n",
    "\t\tMARK_USED(index, high, area);\n",
    "\t\tindex += size;\n",
    "\t\tpage += size;\n",
    "\t}\n",
    "\treturn page;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "1. `__rmqueue` 从 `free_area` 里面取出大小合适的block\n",
    "\n",
    "2. `expand` 裁剪block。把block最后一段>=order 的内存返回，其他的放入到对应的list里面\n",
    "\n",
    "-----------------------\n",
    "\n",
    "### free one page\n",
    "\n",
    "```c\n",
    "/*\n",
    " * Freeing function for a buddy system allocator.\n",
    " *\n",
    " * The concept of a buddy system is to maintain direct-mapped table\n",
    " * (containing bit values) for memory blocks of various \"orders\".\n",
    " * The bottom level table contains the map for the smallest allocatable\n",
    " * units of memory (here, pages), and each level above it describes\n",
    " * pairs of units from the levels below, hence, \"buddies\".\n",
    " * At a high level, all that happens here is marking the table entry\n",
    " * at the bottom level available, and propagating the changes upward\n",
    " * as necessary, plus some accounting needed to play nicely with other\n",
    " * parts of the VM system.\n",
    " * At each level, we keep a list of pages, which are heads of continuous\n",
    " * free pages of length of (1 << order) and marked with PG_Private.Page's\n",
    " * order is recorded in page->private field.\n",
    " * So when we are allocating or freeing one, we can derive the state of the\n",
    " * other.  That is, if we allocate a small block, and both were   \n",
    " * free, the remainder of the region must be split into blocks.   \n",
    " * If a block is freed, and its buddy is also free, then this\n",
    " * triggers coalescing into a block of larger size.            \n",
    " *\n",
    " * -- wli\n",
    " */\n",
    "\n",
    "static inline void __free_pages_bulk (struct page *page, struct page *base,\n",
    "\t\tstruct zone *zone, unsigned int order)\n",
    "{\n",
    "\tunsigned long page_idx;\n",
    "\tstruct page *coalesced;\n",
    "\tint order_size = 1 << order;\n",
    "\n",
    "\tif (unlikely(order))\n",
    "\t\tdestroy_compound_page(page, order);\n",
    "\n",
    "\tpage_idx = page - base;\n",
    "\n",
    "\tBUG_ON(page_idx & (order_size - 1));\n",
    "\tBUG_ON(bad_range(zone, page));\n",
    "\n",
    "\tzone->free_pages += order_size;\n",
    "\twhile (order < MAX_ORDER-1) {\n",
    "\t\tstruct free_area *area;\n",
    "\t\tstruct page *buddy;\n",
    "\t\tint buddy_idx;\n",
    "\n",
    "\t\tbuddy_idx = (page_idx ^ (1 << order));\n",
    "\t\tbuddy = base + buddy_idx;\n",
    "\t\tif (bad_range(zone, buddy))\n",
    "\t\t\tbreak;\n",
    "\t\tif (!page_is_buddy(buddy, order))\n",
    "\t\t\tbreak;\n",
    "\t\t/* Move the buddy up one level. */\n",
    "\t\tlist_del(&buddy->lru);\n",
    "\t\tarea = zone->free_area + order;\n",
    "\t\tarea->nr_free--;\n",
    "\t\trmv_page_order(buddy);\n",
    "\t\tpage_idx &= buddy_idx;\n",
    "\t\torder++;\n",
    "\t}\n",
    "\tcoalesced = base + page_idx;\n",
    "\tset_page_order(coalesced, order);\n",
    "\tlist_add(&coalesced->lru, &zone->free_area[order].free_list);\n",
    "\tzone->free_area[order].nr_free++;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "1. 每一个order block的起始地址一定是（1<<order)的整数倍，也就是(1<<(order-1))位置一定是0\n",
    "\n",
    "2. 因此，如果这个block和它同order的相邻block可以合并成（order+1）的block，那么如果这个block在（1<<order）处为0，那么就跟它后面一个同order的block合并，否则就跟它前面一个同order的block合并。因此这里寻找其buddy就直接用XOR(1<<order)。也就是减去（或者加上）（1<<order）。。。NICE！！！\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-CPU Page Frame Cache\n",
    "\n",
    "![percpu01](resources/percpu01.png)\n",
    "![percpu02](resources/percpu02.png)\n",
    "\n",
    "```c\n",
    "struct per_cpu_pages {\n",
    "\tint count;\t\t/* number of pages in the list */\n",
    "\tint low;\t\t/* low watermark, refill needed */\n",
    "\tint high;\t\t/* high watermark, emptying needed */\n",
    "\tint batch;\t\t/* chunk size for buddy add/remove */\n",
    "\tstruct list_head list;\t/* the list of pages */\n",
    "};\n",
    "\n",
    "struct per_cpu_pageset {\n",
    "\tstruct per_cpu_pages pcp[2];\t/* 0: hot.  1: cold */\n",
    "#ifdef CONFIG_NUMA\n",
    "\tunsigned long numa_hit;\t\t/* allocated in intended node */\n",
    "\tunsigned long numa_miss;\t/* allocated in non intended node */\n",
    "\tunsigned long numa_foreign;\t/* was intended here, hit elsewhere */\n",
    "\tunsigned long interleave_hit; \t/* interleaver prefered this zone */\n",
    "\tunsigned long local_node;\t/* allocation from local node */\n",
    "\tunsigned long other_node;\t/* allocation from other node */\n",
    "#endif\n",
    "} ____cacheline_aligned_in_smp;\n",
    "``` \n",
    "\n",
    "1. ZONE info can be get from /proc/zoneinfo\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linux2.6/mm/page_alloc.c\n",
    "\n",
    "### Allocating page frames though the per-CPU page frame caches\n",
    "\n",
    "\n",
    "```c\n",
    "/* \n",
    " * Obtain a specified number of elements from the buddy allocator, all under\n",
    " * a single hold of the lock, for efficiency.  Add them to the supplied list.\n",
    " * Returns the number of new pages which were placed at *list.\n",
    " */\n",
    "static int rmqueue_bulk(struct zone *zone, unsigned int order, \n",
    "\t\t\tunsigned long count, struct list_head *list)\n",
    "{\n",
    "\tunsigned long flags;\n",
    "\tint i;\n",
    "\tint allocated = 0;\n",
    "\tstruct page *page;\n",
    "\t\n",
    "\tspin_lock_irqsave(&zone->lock, flags);\n",
    "\tfor (i = 0; i < count; ++i) {\n",
    "\t\tpage = __rmqueue(zone, order);\n",
    "\t\tif (page == NULL)\n",
    "\t\t\tbreak;\n",
    "\t\tallocated++;\n",
    "\t\tlist_add_tail(&page->lru, list);\n",
    "\t}\n",
    "\tspin_unlock_irqrestore(&zone->lock, flags);\n",
    "\treturn allocated;\n",
    "}\n",
    "\n",
    "\n",
    "/*\n",
    " * Really, prep_compound_page() should be called from __rmqueue_bulk().  But\n",
    " * we cheat by calling it from here, in the order > 0 path.  Saves a branch\n",
    " * or two.\n",
    " */\n",
    "static struct page *\n",
    "buffered_rmqueue(struct zone *zone, int order, int gfp_flags)\n",
    "{\n",
    "\tunsigned long flags;\n",
    "\tstruct page *page = NULL;\n",
    "\tint cold = !!(gfp_flags & __GFP_COLD);\n",
    "\n",
    "\tif (order == 0) {\n",
    "\t\tstruct per_cpu_pages *pcp;\n",
    "\n",
    "\t\tpcp = &zone->pageset[get_cpu()].pcp[cold];\n",
    "\t\tlocal_irq_save(flags);\n",
    "\t\tif (pcp->count <= pcp->low)\n",
    "\t\t\tpcp->count += rmqueue_bulk(zone, 0,\n",
    "\t\t\t\t\t\tpcp->batch, &pcp->list);\n",
    "\t\tif (pcp->count) {\n",
    "\t\t\tpage = list_entry(pcp->list.next, struct page, lru);\n",
    "\t\t\tlist_del(&page->lru);\n",
    "\t\t\tpcp->count--;\n",
    "\t\t}\n",
    "\t\tlocal_irq_restore(flags);\n",
    "\t\tput_cpu();\n",
    "\t}\n",
    "\n",
    "\tif (page == NULL) {\n",
    "\t\tspin_lock_irqsave(&zone->lock, flags);\n",
    "\t\tpage = __rmqueue(zone, order);\n",
    "\t\tspin_unlock_irqrestore(&zone->lock, flags);\n",
    "\t}\n",
    "\n",
    "\tif (page != NULL) {\n",
    "\t\tBUG_ON(bad_range(zone, page));\n",
    "\t\tmod_page_state_zone(zone, pgalloc, 1 << order);\n",
    "\t\tprep_new_page(page, order);\n",
    "\n",
    "\t\tif (gfp_flags & __GFP_ZERO)\n",
    "\t\t\tprep_zero_page(page, order, gfp_flags);\n",
    "\n",
    "\t\tif (order && (gfp_flags & __GFP_COMP))\n",
    "\t\t\tprep_compound_page(page, order);\n",
    "\t}\n",
    "\treturn page;\n",
    "}\n",
    "```\n",
    "\n",
    "1. `int cold = !!(gfp_flags & __GFP_COLD);` small trick to get one bit value with out >>\n",
    "\n",
    "2. ![percpu03](resources/percpu03.png)\n",
    "\n",
    "3. ![percpu04](resources/percpu04.png)\n",
    "\n",
    "------------------\n",
    "\n",
    "### Release page frames to the per-CPU page frame caches\n",
    "\n",
    "```c\n",
    "/*\n",
    " * Free a 0-order page\n",
    " */\n",
    "static void FASTCALL(free_hot_cold_page(struct page *page, int cold));\n",
    "static void fastcall free_hot_cold_page(struct page *page, int cold)\n",
    "{\n",
    "\tstruct zone *zone = page_zone(page);\n",
    "\tstruct per_cpu_pages *pcp;\n",
    "\tunsigned long flags;\n",
    "\n",
    "\tarch_free_page(page, 0);\n",
    "\n",
    "\tkernel_map_pages(page, 1, 0);\n",
    "\tinc_page_state(pgfree);\n",
    "\tif (PageAnon(page))\n",
    "\t\tpage->mapping = NULL;\n",
    "\tfree_pages_check(__FUNCTION__, page);\n",
    "\tpcp = &zone->pageset[get_cpu()].pcp[cold];\n",
    "\tlocal_irq_save(flags);\n",
    "\tif (pcp->count >= pcp->high)\n",
    "\t\tpcp->count -= free_pages_bulk(zone, pcp->batch, &pcp->list, 0);\n",
    "\tlist_add(&page->lru, &pcp->list);\n",
    "\tpcp->count++;\n",
    "\tlocal_irq_restore(flags);\n",
    "\tput_cpu();\n",
    "}\n",
    "```\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Slab Allocator\n",
    "\n",
    "![slab01](resources/slab01.png)\n",
    "![slab02](resources/slab02.png)\n",
    "\n",
    "----------------------\n",
    "\n",
    "### cache mapping\n",
    "\n",
    "![cachemapping01](resources/cachemapping01.png)\n",
    "\n",
    "![cachemapping02](resources/cachemapping02.png)\n",
    "\n",
    "![cachemapping03](resources/cachemapping03.png)\n",
    "\n",
    "![cachemapping04](resources/cachemapping04.png)\n",
    "\n",
    "![cachemapping05](resources/cachemapping05.png)\n",
    "\n",
    "-----------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## slab coloring\n",
    "\n",
    "![slabcolor](resources/slabcolor01.png)\n",
    "![slabcolor](resources/slabcolor02.png)\n",
    "\n",
    "----------------------\n",
    "\n",
    "1. 我们拿到的每个slab，都会是在page对齐的位置，但是并不一定是连续的。\n",
    "\n",
    "2. 对于一个512K，8路，16Byte 长度cacheline的cache来讲，一共是512K / （8*16）= 4096 sets。如果我们分配的slab长度比较小，比如就是4K，一共有 4K / 16B = 256 个cacheline。因此也就只会用到前 256/4096 个sets。导致后面大量的cache用不到，所以引入slab coloring是有用处的\n",
    "\n",
    "3. 当slab很大的时候，这时候再加offset就没什么用了\n",
    "\n",
    "[reference](https://stackoverflow.com/questions/46731933/linux-slab-allocator-and-cache-performance/57345687#57345687)\n",
    "\n",
    "![slabcolor](resources/slabcolor03.png)\n",
    "![slabcolor](resources/slabcolor04.png)\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noncontigious Memory Area Management\n",
    "\n",
    "1. 无论是buddy system 还是 slab 分配，管理的都是physical memory pages。保证的是物理内存的连续性\n",
    "\n",
    "2. 对于大部分的应用，并不需要物理内存的连续性，只要线性内存连续就好了\n",
    "\n",
    "![nonc01](resources/noncontigious01.png)\n",
    "![nonc02](resources/noncontigious02.png)\n",
    "\n",
    "\n",
    "-------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linux2.6/include/linux/vmalloc.h\n",
    "\n",
    "```c\n",
    "/* bits in vm_struct->flags */\n",
    "#define VM_IOREMAP\t0x00000001\t/* ioremap() and friends */\n",
    "#define VM_ALLOC\t0x00000002\t/* vmalloc() */\n",
    "#define VM_MAP\t\t0x00000004\t/* vmap()ed pages */\n",
    "/* bits [20..32] reserved for arch specific ioremap internals */\n",
    "\n",
    "struct vm_struct {\n",
    "\tvoid\t\t\t*addr;\n",
    "\tunsigned long\t\tsize;\n",
    "\tunsigned long\t\tflags;\n",
    "\tstruct page\t\t**pages;\n",
    "\tunsigned int\t\tnr_pages;\n",
    "\tunsigned long\t\tphys_addr;\n",
    "\tstruct vm_struct\t*next;\n",
    "};\n",
    "```\n",
    "\n",
    "1. `vm_struct` 用了记录一段虚拟地址连续的内存所对应的物理pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linux2.6/mm/vmalloc.c\n",
    "\n",
    "```c\n",
    "/**\n",
    " *\t__vmalloc  -  allocate virtually contiguous memory\n",
    " *\n",
    " *\t@size:\t\tallocation size\n",
    " *\t@gfp_mask:\tflags for the page level allocator\n",
    " *\t@prot:\t\tprotection mask for the allocated pages\n",
    " *\n",
    " *\tAllocate enough pages to cover @size from the page level\n",
    " *\tallocator with @gfp_mask flags.  Map them into contiguous\n",
    " *\tkernel virtual space, using a pagetable protection of @prot.\n",
    " */\n",
    "void *__vmalloc(unsigned long size, int gfp_mask, pgprot_t prot)\n",
    "{\n",
    "\tstruct vm_struct *area;\n",
    "\tstruct page **pages;\n",
    "\tunsigned int nr_pages, array_size, i;\n",
    "\n",
    "\tsize = PAGE_ALIGN(size);\n",
    "\tif (!size || (size >> PAGE_SHIFT) > num_physpages)\n",
    "\t\treturn NULL;\n",
    "\n",
    "\tarea = get_vm_area(size, VM_ALLOC);\n",
    "\tif (!area)\n",
    "\t\treturn NULL;\n",
    "\n",
    "\tnr_pages = size >> PAGE_SHIFT;\n",
    "\tarray_size = (nr_pages * sizeof(struct page *));\n",
    "\n",
    "\tarea->nr_pages = nr_pages;\n",
    "\t/* Please note that the recursion is strictly bounded. */\n",
    "\tif (array_size > PAGE_SIZE)\n",
    "\t\tpages = __vmalloc(array_size, gfp_mask, PAGE_KERNEL);\n",
    "\telse\n",
    "\t\tpages = kmalloc(array_size, (gfp_mask & ~__GFP_HIGHMEM));\n",
    "\tarea->pages = pages;\n",
    "\tif (!area->pages) {\n",
    "\t\tremove_vm_area(area->addr);\n",
    "\t\tkfree(area);\n",
    "\t\treturn NULL;\n",
    "\t}\n",
    "\tmemset(area->pages, 0, array_size);\n",
    "\n",
    "\tfor (i = 0; i < area->nr_pages; i++) {\n",
    "\t\tarea->pages[i] = alloc_page(gfp_mask);\n",
    "\t\tif (unlikely(!area->pages[i])) {\n",
    "\t\t\t/* Successfully allocated i pages, free them in __vunmap() */\n",
    "\t\t\tarea->nr_pages = i;\n",
    "\t\t\tgoto fail;\n",
    "\t\t}\n",
    "\t}\n",
    "\t\n",
    "\tif (map_vm_area(area, prot, &pages))\n",
    "\t\tgoto fail;\n",
    "\treturn area->addr;\n",
    "\n",
    "fail:\n",
    "\tvfree(area->addr);\n",
    "\treturn NULL;\n",
    "}\n",
    "```\n",
    "\n",
    "1. 分配一段size大小的虚拟地址连续的内存空间，并申请每一个page。\n",
    "\n",
    "2. 在申请page数组的时候，如果大小小于PAGE_SIZE，用kmalloc分配，否则调用__vmalloc\n",
    "\n",
    "\n",
    "```c\n",
    "struct vm_struct *__get_vm_area(unsigned long size, unsigned long flags,\n",
    "\t\t\t\tunsigned long start, unsigned long end)\n",
    "{\n",
    "\tstruct vm_struct **p, *tmp, *area;\n",
    "\tunsigned long align = 1;\n",
    "\tunsigned long addr;\n",
    "\n",
    "\tif (flags & VM_IOREMAP) {\n",
    "\t\tint bit = fls(size);\n",
    "\n",
    "\t\tif (bit > IOREMAP_MAX_ORDER)\n",
    "\t\t\tbit = IOREMAP_MAX_ORDER;\n",
    "\t\telse if (bit < PAGE_SHIFT)\n",
    "\t\t\tbit = PAGE_SHIFT;\n",
    "\n",
    "\t\talign = 1ul << bit;\n",
    "\t}\n",
    "\taddr = ALIGN(start, align);\n",
    "\n",
    "\tarea = kmalloc(sizeof(*area), GFP_KERNEL);\n",
    "\tif (unlikely(!area))\n",
    "\t\treturn NULL;\n",
    "\n",
    "\t/*\n",
    "\t * We always allocate a guard page.\n",
    "\t */\n",
    "\tsize += PAGE_SIZE;\n",
    "\tif (unlikely(!size)) {\n",
    "\t\tkfree (area);\n",
    "\t\treturn NULL;\n",
    "\t}\n",
    "\n",
    "\twrite_lock(&vmlist_lock);\n",
    "\tfor (p = &vmlist; (tmp = *p) != NULL ;p = &tmp->next) {\n",
    "\t\tif ((unsigned long)tmp->addr < addr) {\n",
    "\t\t\tif((unsigned long)tmp->addr + tmp->size >= addr)\n",
    "\t\t\t\taddr = ALIGN(tmp->size + \n",
    "\t\t\t\t\t     (unsigned long)tmp->addr, align);\n",
    "\t\t\tcontinue;\n",
    "\t\t}\n",
    "\t\tif ((size + addr) < addr)\n",
    "\t\t\tgoto out;\n",
    "\t\tif (size + addr <= (unsigned long)tmp->addr)\n",
    "\t\t\tgoto found;\n",
    "\t\taddr = ALIGN(tmp->size + (unsigned long)tmp->addr, align);\n",
    "\t\tif (addr > end - size)\n",
    "\t\t\tgoto out;\n",
    "\t}\n",
    "\n",
    "found:\n",
    "\tarea->next = *p;\n",
    "\t*p = area;\n",
    "\n",
    "\tarea->flags = flags;\n",
    "\tarea->addr = (void *)addr;\n",
    "\tarea->size = size;\n",
    "\tarea->pages = NULL;\n",
    "\tarea->nr_pages = 0;\n",
    "\tarea->phys_addr = 0;\n",
    "\twrite_unlock(&vmlist_lock);\n",
    "\n",
    "\treturn area;\n",
    "\n",
    "out:\n",
    "\twrite_unlock(&vmlist_lock);\n",
    "\tkfree(area);\n",
    "\tif (printk_ratelimit())\n",
    "\t\tprintk(KERN_WARNING \"allocation failed: out of vmalloc space - use vmalloc=<size> to increase size.\\n\");\n",
    "\treturn NULL;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "1. `vm_list` is in `linux2.6/mm/vmalloc.c` and is a global variable to record all allocated noncontigious memory. this array is sorted by the address.\n",
    "\n",
    "```c\n",
    "DEFINE_RWLOCK(vmlist_lock);\n",
    "struct vm_struct *vmlist;\n",
    "```\n",
    "\n",
    "2. use `map_vm_area` to connect the physical pages to the page table index.\n",
    "\n",
    "```c\n",
    "int map_vm_area(struct vm_struct *area, pgprot_t prot, struct page ***pages)\n",
    "{\n",
    "\tunsigned long address = (unsigned long) area->addr;\n",
    "\tunsigned long end = address + (area->size-PAGE_SIZE);\n",
    "\tunsigned long next;\n",
    "\tpgd_t *pgd;\n",
    "\tint err = 0;\n",
    "\tint i;\n",
    "\n",
    "\tpgd = pgd_offset_k(address);\n",
    "\tspin_lock(&init_mm.page_table_lock);\n",
    "\tfor (i = pgd_index(address); i <= pgd_index(end-1); i++) {\n",
    "\t\tpud_t *pud = pud_alloc(&init_mm, pgd, address);\n",
    "\t\tif (!pud) {\n",
    "\t\t\terr = -ENOMEM;\n",
    "\t\t\tbreak;\n",
    "\t\t}\n",
    "\t\tnext = (address + PGDIR_SIZE) & PGDIR_MASK;\n",
    "\t\tif (next < address || next > end)\n",
    "\t\t\tnext = end;\n",
    "\t\tif (map_area_pud(pud, address, next, prot, pages)) {\n",
    "\t\t\terr = -ENOMEM;\n",
    "\t\t\tbreak;\n",
    "\t\t}\n",
    "\n",
    "\t\taddress = next;\n",
    "\t\tpgd++;\n",
    "\t}\n",
    "\n",
    "\tspin_unlock(&init_mm.page_table_lock);\n",
    "\tflush_cache_vmap((unsigned long) area->addr, end);\n",
    "\treturn err;\n",
    "}\n",
    "```\n",
    "\n",
    "1. `next = (address + PGDIR_SIZE) & PGDIR_MASK;` 第一级是`PGDIR_SIZE = (1<<26)`的\n",
    "\n",
    "2. 下面这些函数逐级在page table上建立item\n",
    "\n",
    "```c\n",
    "static int map_area_pte(pte_t *pte, unsigned long address,\n",
    "\t\t\t       unsigned long size, pgprot_t prot,\n",
    "\t\t\t       struct page ***pages)\n",
    "{\n",
    "\tunsigned long end;\n",
    "\n",
    "\taddress &= ~PMD_MASK;\n",
    "\tend = address + size;\n",
    "\tif (end > PMD_SIZE)\n",
    "\t\tend = PMD_SIZE;\n",
    "\n",
    "\tdo {\n",
    "\t\tstruct page *page = **pages;\n",
    "\t\tWARN_ON(!pte_none(*pte));\n",
    "\t\tif (!page)\n",
    "\t\t\treturn -ENOMEM;\n",
    "\n",
    "\t\tset_pte(pte, mk_pte(page, prot));\n",
    "\t\taddress += PAGE_SIZE;\n",
    "\t\tpte++;\n",
    "\t\t(*pages)++;\n",
    "\t} while (address < end);\n",
    "\treturn 0;\n",
    "}\n",
    "\n",
    "static int map_area_pmd(pmd_t *pmd, unsigned long address,\n",
    "\t\t\t       unsigned long size, pgprot_t prot,\n",
    "\t\t\t       struct page ***pages)\n",
    "{\n",
    "\tunsigned long base, end;\n",
    "\n",
    "\tbase = address & PUD_MASK;\n",
    "\taddress &= ~PUD_MASK;\n",
    "\tend = address + size;\n",
    "\tif (end > PUD_SIZE)\n",
    "\t\tend = PUD_SIZE;\n",
    "\n",
    "\tdo {\n",
    "\t\tpte_t * pte = pte_alloc_kernel(&init_mm, pmd, base + address);\n",
    "\t\tif (!pte)\n",
    "\t\t\treturn -ENOMEM;\n",
    "\t\tif (map_area_pte(pte, address, end - address, prot, pages))\n",
    "\t\t\treturn -ENOMEM;\n",
    "\t\taddress = (address + PMD_SIZE) & PMD_MASK;\n",
    "\t\tpmd++;\n",
    "\t} while (address < end);\n",
    "\n",
    "\treturn 0;\n",
    "}\n",
    "\n",
    "static int map_area_pud(pud_t *pud, unsigned long address,\n",
    "\t\t\t       unsigned long end, pgprot_t prot,\n",
    "\t\t\t       struct page ***pages)\n",
    "{\n",
    "\tdo {\n",
    "\t\tpmd_t *pmd = pmd_alloc(&init_mm, pud, address);\n",
    "\t\tif (!pmd)\n",
    "\t\t\treturn -ENOMEM;\n",
    "\t\tif (map_area_pmd(pmd, address, end - address, prot, pages))\n",
    "\t\t\treturn -ENOMEM;\n",
    "\t\taddress = (address + PUD_SIZE) & PUD_MASK;\n",
    "\t\tpud++;\n",
    "\t} while (address && address < end);\n",
    "\n",
    "\treturn 0;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paging directory\n",
    "\n",
    "![paging01](resources/paging01.png)\n",
    "![paging02](resources/paging02.png)\n",
    "![paging03](resources/paging03.png)\n",
    "![paging03](resources/pagedesp03.png)\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Frame Management\n",
    "\n",
    "### page descriptors\n",
    "\n",
    "![pagedesp01](resources/pagedesp01.png)\n",
    "![pagedesp02](resources/pagedesp02.png)\n",
    "\n",
    "1. `mem_map` 是个 `page` 的array。在numa架构下，\n",
    "\n",
    "### linux2.6/mm/memory.c\n",
    "\n",
    "```c\n",
    "#ifndef CONFIG_DISCONTIGMEM\n",
    "/* use the per-pgdat data instead for discontigmem - mbligh */\n",
    "unsigned long max_mapnr;\n",
    "struct page *mem_map;\n",
    "\n",
    "EXPORT_SYMBOL(max_mapnr);\n",
    "EXPORT_SYMBOL(mem_map);\n",
    "#endif\n",
    "\n",
    "```\n",
    "\n",
    "### linux2.6/include/linux/mm.h\n",
    "\n",
    "```c\n",
    "/*\n",
    " * Each physical page in the system has a struct page associated with\n",
    " * it to keep track of whatever it is we are using the page for at the\n",
    " * moment. Note that we have no way to track which tasks are using\n",
    " * a page.\n",
    " */\n",
    "struct page {\n",
    "\tpage_flags_t flags;\t\t/* Atomic flags, some possibly\n",
    "\t\t\t\t\t * updated asynchronously */\n",
    "\tatomic_t _count;\t\t/* Usage count, see below. */\n",
    "\tatomic_t _mapcount;\t\t/* Count of ptes mapped in mms,\n",
    "\t\t\t\t\t * to show when page is mapped\n",
    "\t\t\t\t\t * & limit reverse map searches.\n",
    "\t\t\t\t\t */\n",
    "\tunsigned long private;\t\t/* Mapping-private opaque data:\n",
    "\t\t\t\t\t * usually used for buffer_heads\n",
    "\t\t\t\t\t * if PagePrivate set; used for\n",
    "\t\t\t\t\t * swp_entry_t if PageSwapCache\n",
    "\t\t\t\t\t * When page is free, this indicates\n",
    "\t\t\t\t\t * order in the buddy system.\n",
    "\t\t\t\t\t */\n",
    "\tstruct address_space *mapping;\t/* If low bit clear, points to\n",
    "\t\t\t\t\t * inode address_space, or NULL.\n",
    "\t\t\t\t\t * If page mapped as anonymous\n",
    "\t\t\t\t\t * memory, low bit is set, and\n",
    "\t\t\t\t\t * it points to anon_vma object:\n",
    "\t\t\t\t\t * see PAGE_MAPPING_ANON below.\n",
    "\t\t\t\t\t */\n",
    "\tpgoff_t index;\t\t\t/* Our offset within mapping. */\n",
    "\tstruct list_head lru;\t\t/* Pageout list, eg. active_list\n",
    "\t\t\t\t\t * protected by zone->lru_lock !\n",
    "\t\t\t\t\t */\n",
    "\t/*\n",
    "\t * On machines where all RAM is mapped into kernel address space,\n",
    "\t * we can simply calculate the virtual address. On machines with\n",
    "\t * highmem some memory is mapped into kernel virtual memory\n",
    "\t * dynamically, so we need a place to store that address.\n",
    "\t * Note that this field could be 16 bits on x86 ... ;)\n",
    "\t *\n",
    "\t * Architectures with slow multiplication can define\n",
    "\t * WANT_PAGE_VIRTUAL in asm/page.h\n",
    "\t */\n",
    "#if defined(WANT_PAGE_VIRTUAL)\n",
    "\tvoid *virtual;\t\t\t/* Kernel virtual address (NULL if\n",
    "\t\t\t\t\t   not kmapped, ie. highmem) */\n",
    "#endif /* WANT_PAGE_VIRTUAL */\n",
    "};\n",
    "\n",
    "```\n",
    "\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUMA\n",
    "\n",
    "![numa01](resources/numa01.png)\n",
    "![numa02](resources/numa02.png)\n",
    "\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Addrss Space\n",
    "\n",
    "![pas01](resources/processaddressspace01.png)\n",
    "\n",
    "1. The kernel represents the intervals of linear addresses by means of resources called 'memory region'. In code it is `struct vm_area_struct`.\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### memory descriptor\n",
    "\n",
    "![memdesp01](resources/memorydesp01.png)\n",
    "![memdesp02](resources/memorydesp02.png)\n",
    "![memdesp03](resources/memorydesp03.png)\n",
    "![memdesp04](resources/memorydesp04.png)\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linux2.6/include/linux/mm.h\n",
    "\n",
    "```c\n",
    "/*\n",
    " * This struct defines a memory VMM memory area. There is one of these\n",
    " * per VM-area/task.  A VM area is any part of the process virtual memory\n",
    " * space that has a special rule for the page-fault handlers (ie a shared\n",
    " * library, the executable area etc).\n",
    " */\n",
    "struct vm_area_struct {\n",
    "\tstruct mm_struct * vm_mm;\t/* The address space we belong to. */\n",
    "\tunsigned long vm_start;\t\t/* Our start address within vm_mm. */\n",
    "\tunsigned long vm_end;\t\t/* The first byte after our end address\n",
    "\t\t\t\t\t   within vm_mm. */\n",
    "\n",
    "\t/* linked list of VM areas per task, sorted by address */\n",
    "\tstruct vm_area_struct *vm_next;\n",
    "\n",
    "\tpgprot_t vm_page_prot;\t\t/* Access permissions of this VMA. */\n",
    "\tunsigned long vm_flags;\t\t/* Flags, listed below. */\n",
    "\n",
    "\tstruct rb_node vm_rb;\n",
    "\n",
    "\t/*\n",
    "\t * For areas with an address space and backing store,\n",
    "\t * linkage into the address_space->i_mmap prio tree, or\n",
    "\t * linkage to the list of like vmas hanging off its node, or\n",
    "\t * linkage of vma in the address_space->i_mmap_nonlinear list.\n",
    "\t */\n",
    "\tunion {\n",
    "\t\tstruct {\n",
    "\t\t\tstruct list_head list;\n",
    "\t\t\tvoid *parent;\t/* aligns with prio_tree_node parent */\n",
    "\t\t\tstruct vm_area_struct *head;\n",
    "\t\t} vm_set;\n",
    "\n",
    "\t\tstruct raw_prio_tree_node prio_tree_node;\n",
    "\t} shared;\n",
    "\n",
    "\t/*\n",
    "\t * A file's MAP_PRIVATE vma can be in both i_mmap tree and anon_vma\n",
    "\t * list, after a COW of one of the file pages.  A MAP_SHARED vma\n",
    "\t * can only be in the i_mmap tree.  An anonymous MAP_PRIVATE, stack\n",
    "\t * or brk vma (with NULL file) can only be in an anon_vma list.\n",
    "\t */\n",
    "\tstruct list_head anon_vma_node;\t/* Serialized by anon_vma->lock */\n",
    "\tstruct anon_vma *anon_vma;\t/* Serialized by page_table_lock */\n",
    "\n",
    "\t/* Function pointers to deal with this struct. */\n",
    "\tstruct vm_operations_struct * vm_ops;\n",
    "\n",
    "\t/* Information about our backing store: */\n",
    "\tunsigned long vm_pgoff;\t\t/* Offset (within vm_file) in PAGE_SIZE\n",
    "\t\t\t\t\t   units, *not* PAGE_CACHE_SIZE */\n",
    "\tstruct file * vm_file;\t\t/* File we map to (can be NULL). */\n",
    "\tvoid * vm_private_data;\t\t/* was vm_pte (shared mem) */\n",
    "\tunsigned long vm_truncate_count;/* truncate_count or restart_addr */\n",
    "\n",
    "#ifndef CONFIG_MMU\n",
    "\tatomic_t vm_usage;\t\t/* refcount (VMAs shared if !MMU) */\n",
    "#endif\n",
    "#ifdef CONFIG_NUMA\n",
    "\tstruct mempolicy *vm_policy;\t/* NUMA policy for the VMA */\n",
    "#endif\n",
    "};\n",
    "\n",
    "```\n",
    "\n",
    "![memoryregion01](resources/memoryregion01.png)\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memory region flags\n",
    "\n",
    "![mrf01](resources/memoryregionflag01.png)\n",
    "\n",
    "---------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memory region handling\n",
    "\n",
    "### linux2.6/mm/mmap.c\n",
    "\n",
    "```c\n",
    "/* Look up the first VMA which satisfies  addr < vm_end,  NULL if none. */\n",
    "struct vm_area_struct * find_vma(struct mm_struct * mm, unsigned long addr)\n",
    "{\n",
    "\tstruct vm_area_struct *vma = NULL;\n",
    "\n",
    "\tif (mm) {\n",
    "\t\t/* Check the cache first. */\n",
    "\t\t/* (Cache hit rate is typically around 35%.) */\n",
    "\t\tvma = mm->mmap_cache;\n",
    "\t\tif (!(vma && vma->vm_end > addr && vma->vm_start <= addr)) {\n",
    "\t\t\tstruct rb_node * rb_node;\n",
    "\n",
    "\t\t\trb_node = mm->mm_rb.rb_node;\n",
    "\t\t\tvma = NULL;\n",
    "\n",
    "\t\t\twhile (rb_node) {\n",
    "\t\t\t\tstruct vm_area_struct * vma_tmp;\n",
    "\n",
    "\t\t\t\tvma_tmp = rb_entry(rb_node,\n",
    "\t\t\t\t\t\tstruct vm_area_struct, vm_rb);\n",
    "\n",
    "\t\t\t\tif (vma_tmp->vm_end > addr) {\n",
    "\t\t\t\t\tvma = vma_tmp;\n",
    "\t\t\t\t\tif (vma_tmp->vm_start <= addr)\n",
    "\t\t\t\t\t\tbreak;\n",
    "\t\t\t\t\trb_node = rb_node->rb_left;\n",
    "\t\t\t\t} else\n",
    "\t\t\t\t\trb_node = rb_node->rb_right;\n",
    "\t\t\t}\n",
    "\t\t\tif (vma)\n",
    "\t\t\t\tmm->mmap_cache = vma;\n",
    "\t\t}\n",
    "\t}\n",
    "\treturn vma;\n",
    "}\n",
    "```\n",
    "\n",
    "1. It locates the first memory region whose `vm_end` field if greater than `addr` and returns the address of its descriptor.\n",
    "\n",
    "2. Each memory descriptor includes an `mmap_cache` field that stores the descriptor address of the region that was last referenced by the process. Locality of address references in programs makes it highly likely that if the last linear address checked beloned to a given region, the next one to be checked belongs to the same region.\n",
    "\n",
    "\n",
    "```c\n",
    "/* Same as find_vma, but also return a pointer to the previous VMA in *pprev. */\n",
    "struct vm_area_struct *\n",
    "find_vma_prev(struct mm_struct *mm, unsigned long addr,\n",
    "\t\t\tstruct vm_area_struct **pprev)\n",
    "{\n",
    "\tstruct vm_area_struct *vma = NULL, *prev = NULL;\n",
    "\tstruct rb_node * rb_node;\n",
    "\tif (!mm)\n",
    "\t\tgoto out;\n",
    "\n",
    "\t/* Guard against addr being lower than the first VMA */\n",
    "\tvma = mm->mmap;\n",
    "\n",
    "\t/* Go through the RB tree quickly. */\n",
    "\trb_node = mm->mm_rb.rb_node;\n",
    "\n",
    "\twhile (rb_node) {\n",
    "\t\tstruct vm_area_struct *vma_tmp;\n",
    "\t\tvma_tmp = rb_entry(rb_node, struct vm_area_struct, vm_rb);\n",
    "\n",
    "\t\tif (addr < vma_tmp->vm_end) {\n",
    "\t\t\trb_node = rb_node->rb_left;\n",
    "\t\t} else {\n",
    "\t\t\tprev = vma_tmp;\n",
    "\t\t\tif (!prev->vm_next || (addr < prev->vm_next->vm_end))\n",
    "\t\t\t\tbreak;\n",
    "\t\t\trb_node = rb_node->rb_right;\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "out:\n",
    "\t*pprev = prev;\n",
    "\treturn prev ? prev->vm_next : vma;\n",
    "}\n",
    "```\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_unmapped_area()\n",
    "\n",
    "![getunmapped01](resources/getunmapped01.png)\n",
    "\n",
    "Actually they are the `heap` and `stack` memory\n",
    "\n",
    "\n",
    "**1. allocate memory from lower -> higher (heap)**\n",
    "\n",
    "```c\n",
    "/* Get an address range which is currently unmapped.\n",
    " * For shmat() with addr=0.\n",
    " *\n",
    " * Ugly calling convention alert:\n",
    " * Return value with the low bits set means error value,\n",
    " * ie\n",
    " *\tif (ret & ~PAGE_MASK)\n",
    " *\t\terror = ret;\n",
    " *\n",
    " * This function \"knows\" that -ENOMEM has the bits set.\n",
    " */\n",
    "#ifndef HAVE_ARCH_UNMAPPED_AREA\n",
    "unsigned long\n",
    "arch_get_unmapped_area(struct file *filp, unsigned long addr,\n",
    "\t\tunsigned long len, unsigned long pgoff, unsigned long flags)\n",
    "{\n",
    "\tstruct mm_struct *mm = current->mm;\n",
    "\tstruct vm_area_struct *vma;\n",
    "\tunsigned long start_addr;\n",
    "\n",
    "\tif (len > TASK_SIZE)\n",
    "\t\treturn -ENOMEM;\n",
    "\n",
    "\tif (addr) {\n",
    "\t\taddr = PAGE_ALIGN(addr);\n",
    "\t\tvma = find_vma(mm, addr);\n",
    "\t\tif (TASK_SIZE - len >= addr &&\n",
    "\t\t    (!vma || addr + len <= vma->vm_start))\n",
    "\t\t\treturn addr;\n",
    "\t}\n",
    "\tstart_addr = addr = mm->free_area_cache;\n",
    "\n",
    "full_search:\n",
    "\tfor (vma = find_vma(mm, addr); ; vma = vma->vm_next) {\n",
    "\t\t/* At this point:  (!vma || addr < vma->vm_end). */\n",
    "\t\tif (TASK_SIZE - len < addr) {\n",
    "\t\t\t/*\n",
    "\t\t\t * Start a new search - just in case we missed\n",
    "\t\t\t * some holes.\n",
    "\t\t\t */\n",
    "\t\t\tif (start_addr != TASK_UNMAPPED_BASE) {\n",
    "\t\t\t\tstart_addr = addr = TASK_UNMAPPED_BASE;\n",
    "\t\t\t\tgoto full_search;\n",
    "\t\t\t}\n",
    "\t\t\treturn -ENOMEM;\n",
    "\t\t}\n",
    "\t\tif (!vma || addr + len <= vma->vm_start) {\n",
    "\t\t\t/*\n",
    "\t\t\t * Remember the place where we stopped the search:\n",
    "\t\t\t */\n",
    "\t\t\tmm->free_area_cache = addr + len;\n",
    "\t\t\treturn addr;\n",
    "\t\t}\n",
    "\t\taddr = vma->vm_end;\n",
    "\t}\n",
    "}\n",
    "#endif\t\n",
    "\n",
    "```\n",
    "\n",
    "1. Firstly search the `vma` just behind `addr` and check it. If found it, just return. The `addr` may be from the `mm->free_area_cache`\n",
    "\n",
    "2. If it's not found, begin the full search from the beginning. Find the first and record the address to the `free_area_cache`\n",
    "\n",
    "3. `addr + len` must below the `TASK_SIZE`. User space process can only use low address space\n",
    "\n",
    "\n",
    "**2. allocate memory from high -> low (stack)**\n",
    "\n",
    "```c\n",
    "/*\n",
    " * This mmap-allocator allocates new areas top-down from below the\n",
    " * stack's low limit (the base):\n",
    " */\n",
    "#ifndef HAVE_ARCH_UNMAPPED_AREA_TOPDOWN\n",
    "unsigned long\n",
    "arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,\n",
    "\t\t\t  const unsigned long len, const unsigned long pgoff,\n",
    "\t\t\t  const unsigned long flags)\n",
    "{\n",
    "\tstruct vm_area_struct *vma, *prev_vma;\n",
    "\tstruct mm_struct *mm = current->mm;\n",
    "\tunsigned long base = mm->mmap_base, addr = addr0;\n",
    "\tint first_time = 1;\n",
    "\n",
    "\t/* requested length too big for entire address space */\n",
    "\tif (len > TASK_SIZE)\n",
    "\t\treturn -ENOMEM;\n",
    "\n",
    "\t/* dont allow allocations above current base */\n",
    "\tif (mm->free_area_cache > base)\n",
    "\t\tmm->free_area_cache = base;\n",
    "\n",
    "\t/* requesting a specific address */\n",
    "\tif (addr) {\n",
    "\t\taddr = PAGE_ALIGN(addr);\n",
    "\t\tvma = find_vma(mm, addr);\n",
    "\t\tif (TASK_SIZE - len >= addr &&\n",
    "\t\t\t\t(!vma || addr + len <= vma->vm_start))\n",
    "\t\t\treturn addr;\n",
    "\t}\n",
    "\n",
    "try_again:\n",
    "\t/* make sure it can fit in the remaining address space */\n",
    "\tif (mm->free_area_cache < len)\n",
    "\t\tgoto fail;\n",
    "\n",
    "\t/* either no address requested or cant fit in requested address hole */\n",
    "\taddr = (mm->free_area_cache - len) & PAGE_MASK;\n",
    "\tdo {\n",
    "\t\t/*\n",
    "\t\t * Lookup failure means no vma is above this address,\n",
    "\t\t * i.e. return with success:\n",
    "\t\t */\n",
    " \t \tif (!(vma = find_vma_prev(mm, addr, &prev_vma)))\n",
    "\t\t\treturn addr;\n",
    "\n",
    "\t\t/*\n",
    "\t\t * new region fits between prev_vma->vm_end and\n",
    "\t\t * vma->vm_start, use it:\n",
    "\t\t */\n",
    "\t\tif (addr+len <= vma->vm_start &&\n",
    "\t\t\t\t(!prev_vma || (addr >= prev_vma->vm_end)))\n",
    "\t\t\t/* remember the address as a hint for next time */\n",
    "\t\t\treturn (mm->free_area_cache = addr);\n",
    "\t\telse\n",
    "\t\t\t/* pull free_area_cache down to the first hole */\n",
    "\t\t\tif (mm->free_area_cache == vma->vm_end)\n",
    "\t\t\t\tmm->free_area_cache = vma->vm_start;\n",
    "\n",
    "\t\t/* try just below the current vma->vm_start */\n",
    "\t\taddr = vma->vm_start-len;\n",
    "\t} while (len <= vma->vm_start);\n",
    "\n",
    "fail:\n",
    "\t/*\n",
    "\t * if hint left us with no space for the requested\n",
    "\t * mapping then try again:\n",
    "\t */\n",
    "\tif (first_time) {\n",
    "\t\tmm->free_area_cache = base;\n",
    "\t\tfirst_time = 0;\n",
    "\t\tgoto try_again;\n",
    "\t}\n",
    "\t/*\n",
    "\t * A failed mmap() very likely causes application failure,\n",
    "\t * so fall back to the bottom-up function here. This scenario\n",
    "\t * can happen with large stack limits and large mmap()\n",
    "\t * allocations.\n",
    "\t */\n",
    "\tmm->free_area_cache = TASK_UNMAPPED_BASE;\n",
    "\taddr = arch_get_unmapped_area(filp, addr0, len, pgoff, flags);\n",
    "\t/*\n",
    "\t * Restore the topdown base:\n",
    "\t */\n",
    "\tmm->free_area_cache = base;\n",
    "\n",
    "\treturn addr;\n",
    "}\n",
    "#endif\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "1. `base` 就是栈底（高地址），保证分配地址不会超过栈底\n",
    "\n",
    "```c\n",
    "\t/* dont allow allocations above current base */\n",
    "\tif (mm->free_area_cache > base)\n",
    "\t\tmm->free_area_cache = base;\n",
    "```\n",
    "\n",
    "2. 当前面已经没有vma的时候，返回addr\n",
    "\n",
    "```c\n",
    "\t\t/*\n",
    "\t\t * Lookup failure means no vma is above this address,\n",
    "\t\t * i.e. return with success:\n",
    "\t\t */\n",
    " \t \tif (!(vma = find_vma_prev(mm, addr, &prev_vma)))\n",
    "\t\t\treturn addr;\n",
    "```\n",
    "\n",
    "\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mmap01](resources/mmap01.png)\n",
    "![mmap02](resources/mmap01.png)\n",
    "[reference](https://zhuanlan.zhihu.com/p/539582361)\n",
    "\n",
    "### linux2.6/include/linux/mm.h\n",
    "\n",
    "```c\n",
    "#define PAGE_SIZE\t\t(1UL << PAGE_SHIFT)\n",
    "#define PAGE_MASK\t\t(~(PAGE_SIZE-1))\n",
    "\n",
    "static inline unsigned long do_mmap(struct file *file, unsigned long addr,\n",
    "\tunsigned long len, unsigned long prot,\n",
    "\tunsigned long flag, unsigned long offset)\n",
    "{\n",
    "\tunsigned long ret = -EINVAL;\n",
    "\tif ((offset + PAGE_ALIGN(len)) < offset)\n",
    "\t\tgoto out;\n",
    "\tif (!(offset & ~PAGE_MASK))\n",
    "\t\tret = do_mmap_pgoff(file, addr, len, prot, flag, offset >> PAGE_SHIFT);\n",
    "out:\n",
    "\treturn ret;\n",
    "}\n",
    "```\n",
    "\n",
    "1. `if ((offset + PAGE_ALIGN(len)) < offset)` 溢出检查\n",
    "\n",
    "2. `offset` 必须 `PAGE_SIZE` 对齐\n",
    "\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linux2.6/mm/mmap.c\n",
    "\n",
    "```c\n",
    "/*\n",
    " * The caller must hold down_write(current->mm->mmap_sem).\n",
    " */\n",
    "\n",
    "unsigned long do_mmap_pgoff(struct file * file, unsigned long addr,\n",
    "\t\t\tunsigned long len, unsigned long prot,\n",
    "\t\t\tunsigned long flags, unsigned long pgoff)\n",
    "{\n",
    "\tstruct mm_struct * mm = current->mm;\n",
    "\tstruct vm_area_struct * vma, * prev;\n",
    "\tstruct inode *inode;\n",
    "\tunsigned int vm_flags;\n",
    "\tint correct_wcount = 0;\n",
    "\tint error;\n",
    "\tstruct rb_node ** rb_link, * rb_parent;\n",
    "\tint accountable = 1;\n",
    "\tunsigned long charged = 0;\n",
    "\n",
    "\tif (file) {\n",
    "\t\tif (is_file_hugepages(file))\n",
    "\t\t\taccountable = 0;\n",
    "\n",
    "\t\tif (!file->f_op || !file->f_op->mmap)\n",
    "\t\t\treturn -ENODEV;\n",
    "\n",
    "\t\tif ((prot & PROT_EXEC) &&\n",
    "\t\t    (file->f_vfsmnt->mnt_flags & MNT_NOEXEC))\n",
    "\t\t\treturn -EPERM;\n",
    "\t}\n",
    "\t/*\n",
    "\t * Does the application expect PROT_READ to imply PROT_EXEC?\n",
    "\t *\n",
    "\t * (the exception is when the underlying filesystem is noexec\n",
    "\t *  mounted, in which case we dont add PROT_EXEC.)\n",
    "\t */\n",
    "\tif ((prot & PROT_READ) && (current->personality & READ_IMPLIES_EXEC))\n",
    "\t\tif (!(file && (file->f_vfsmnt->mnt_flags & MNT_NOEXEC)))\n",
    "\t\t\tprot |= PROT_EXEC;\n",
    "\n",
    "\tif (!len)\n",
    "\t\treturn addr;\n",
    "\n",
    "\t/* Careful about overflows.. */\n",
    "\tlen = PAGE_ALIGN(len);\n",
    "\tif (!len || len > TASK_SIZE)\n",
    "\t\treturn -EINVAL;\n",
    "\n",
    "\t/* offset overflow? */\n",
    "\tif ((pgoff + (len >> PAGE_SHIFT)) < pgoff)\n",
    "\t\treturn -EINVAL;\n",
    "\n",
    "\t/* Too many mappings? */\n",
    "\tif (mm->map_count > sysctl_max_map_count)\n",
    "\t\treturn -ENOMEM;\n",
    "\n",
    "\t/* Obtain the address to map to. we verify (or select) it and ensure\n",
    "\t * that it represents a valid section of the address space.\n",
    "\t */\n",
    "\taddr = get_unmapped_area(file, addr, len, pgoff, flags);\n",
    "\tif (addr & ~PAGE_MASK)\n",
    "\t\treturn addr;\n",
    "\n",
    "\t/* Do simple checking here so the lower-level routines won't have\n",
    "\t * to. we assume access permissions have been handled by the open\n",
    "\t * of the memory object, so we don't do any here.\n",
    "\t */\n",
    "\tvm_flags = calc_vm_prot_bits(prot) | calc_vm_flag_bits(flags) |\n",
    "\t\t\tmm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;\n",
    "\n",
    "\tif (flags & MAP_LOCKED) {\n",
    "\t\tif (!can_do_mlock())\n",
    "\t\t\treturn -EPERM;\n",
    "\t\tvm_flags |= VM_LOCKED;\n",
    "\t}\n",
    "\t/* mlock MCL_FUTURE? */\n",
    "\tif (vm_flags & VM_LOCKED) {\n",
    "\t\tunsigned long locked, lock_limit;\n",
    "\t\tlocked = mm->locked_vm << PAGE_SHIFT;\n",
    "\t\tlock_limit = current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur;\n",
    "\t\tlocked += len;\n",
    "\t\tif (locked > lock_limit && !capable(CAP_IPC_LOCK))\n",
    "\t\t\treturn -EAGAIN;\n",
    "\t}\n",
    "\n",
    "\tinode = file ? file->f_dentry->d_inode : NULL;\n",
    "\n",
    "\tif (file) {\n",
    "\t\tswitch (flags & MAP_TYPE) {\n",
    "\t\tcase MAP_SHARED:\n",
    "\t\t\tif ((prot&PROT_WRITE) && !(file->f_mode&FMODE_WRITE))\n",
    "\t\t\t\treturn -EACCES;\n",
    "\n",
    "\t\t\t/*\n",
    "\t\t\t * Make sure we don't allow writing to an append-only\n",
    "\t\t\t * file..\n",
    "\t\t\t */\n",
    "\t\t\tif (IS_APPEND(inode) && (file->f_mode & FMODE_WRITE))\n",
    "\t\t\t\treturn -EACCES;\n",
    "\n",
    "\t\t\t/*\n",
    "\t\t\t * Make sure there are no mandatory locks on the file.\n",
    "\t\t\t */\n",
    "\t\t\tif (locks_verify_locked(inode))\n",
    "\t\t\t\treturn -EAGAIN;\n",
    "\n",
    "\t\t\tvm_flags |= VM_SHARED | VM_MAYSHARE;\n",
    "\t\t\tif (!(file->f_mode & FMODE_WRITE))\n",
    "\t\t\t\tvm_flags &= ~(VM_MAYWRITE | VM_SHARED);\n",
    "\n",
    "\t\t\t/* fall through */\n",
    "\t\tcase MAP_PRIVATE:\n",
    "\t\t\tif (!(file->f_mode & FMODE_READ))\n",
    "\t\t\t\treturn -EACCES;\n",
    "\t\t\tbreak;\n",
    "\n",
    "\t\tdefault:\n",
    "\t\t\treturn -EINVAL;\n",
    "\t\t}\n",
    "\t} else {\n",
    "\t\tswitch (flags & MAP_TYPE) {\n",
    "\t\tcase MAP_SHARED:\n",
    "\t\t\tvm_flags |= VM_SHARED | VM_MAYSHARE;\n",
    "\t\t\tbreak;\n",
    "\t\tcase MAP_PRIVATE:\n",
    "\t\t\t/*\n",
    "\t\t\t * Set pgoff according to addr for anon_vma.\n",
    "\t\t\t */\n",
    "\t\t\tpgoff = addr >> PAGE_SHIFT;\n",
    "\t\t\tbreak;\n",
    "\t\tdefault:\n",
    "\t\t\treturn -EINVAL;\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\terror = security_file_mmap(file, prot, flags);\n",
    "\tif (error)\n",
    "\t\treturn error;\n",
    "\t\t\n",
    "\t/* Clear old maps */\n",
    "\terror = -ENOMEM;\n",
    "munmap_back:\n",
    "\tvma = find_vma_prepare(mm, addr, &prev, &rb_link, &rb_parent);\n",
    "\tif (vma && vma->vm_start < addr + len) {\n",
    "\t\tif (do_munmap(mm, addr, len))\n",
    "\t\t\treturn -ENOMEM;\n",
    "\t\tgoto munmap_back;\n",
    "\t}\n",
    "\n",
    "\t/* Check against address space limit. */\n",
    "\tif ((mm->total_vm << PAGE_SHIFT) + len\n",
    "\t    > current->signal->rlim[RLIMIT_AS].rlim_cur)\n",
    "\t\treturn -ENOMEM;\n",
    "\n",
    "\tif (accountable && (!(flags & MAP_NORESERVE) ||\n",
    "\t\t\t    sysctl_overcommit_memory == OVERCOMMIT_NEVER)) {\n",
    "\t\tif (vm_flags & VM_SHARED) {\n",
    "\t\t\t/* Check memory availability in shmem_file_setup? */\n",
    "\t\t\tvm_flags |= VM_ACCOUNT;\n",
    "\t\t} else if (vm_flags & VM_WRITE) {\n",
    "\t\t\t/*\n",
    "\t\t\t * Private writable mapping: check memory availability\n",
    "\t\t\t */\n",
    "\t\t\tcharged = len >> PAGE_SHIFT;\n",
    "\t\t\tif (security_vm_enough_memory(charged))\n",
    "\t\t\t\treturn -ENOMEM;\n",
    "\t\t\tvm_flags |= VM_ACCOUNT;\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\t/*\n",
    "\t * Can we just expand an old private anonymous mapping?\n",
    "\t * The VM_SHARED test is necessary because shmem_zero_setup\n",
    "\t * will create the file object for a shared anonymous map below.\n",
    "\t */\n",
    "\tif (!file && !(vm_flags & VM_SHARED) &&\n",
    "\t    vma_merge(mm, prev, addr, addr + len, vm_flags,\n",
    "\t\t\t\t\tNULL, NULL, pgoff, NULL))\n",
    "\t\tgoto out;\n",
    "\n",
    "\t/*\n",
    "\t * Determine the object being mapped and call the appropriate\n",
    "\t * specific mapper. the address has already been validated, but\n",
    "\t * not unmapped, but the maps are removed from the list.\n",
    "\t */\n",
    "\tvma = kmem_cache_alloc(vm_area_cachep, SLAB_KERNEL);\n",
    "\tif (!vma) {\n",
    "\t\terror = -ENOMEM;\n",
    "\t\tgoto unacct_error;\n",
    "\t}\n",
    "\tmemset(vma, 0, sizeof(*vma));\n",
    "\n",
    "\tvma->vm_mm = mm;\n",
    "\tvma->vm_start = addr;\n",
    "\tvma->vm_end = addr + len;\n",
    "\tvma->vm_flags = vm_flags;\n",
    "\tvma->vm_page_prot = protection_map[vm_flags & 0x0f];\n",
    "\tvma->vm_pgoff = pgoff;\n",
    "\n",
    "\tif (file) {\n",
    "\t\terror = -EINVAL;\n",
    "\t\tif (vm_flags & (VM_GROWSDOWN|VM_GROWSUP))\n",
    "\t\t\tgoto free_vma;\n",
    "\t\tif (vm_flags & VM_DENYWRITE) {\n",
    "\t\t\terror = deny_write_access(file);\n",
    "\t\t\tif (error)\n",
    "\t\t\t\tgoto free_vma;\n",
    "\t\t\tcorrect_wcount = 1;\n",
    "\t\t}\n",
    "\t\tvma->vm_file = file;\n",
    "\t\tget_file(file);\n",
    "\t\terror = file->f_op->mmap(file, vma);\n",
    "\t\tif (error)\n",
    "\t\t\tgoto unmap_and_free_vma;\n",
    "\t} else if (vm_flags & VM_SHARED) {\n",
    "\t\terror = shmem_zero_setup(vma);\n",
    "\t\tif (error)\n",
    "\t\t\tgoto free_vma;\n",
    "\t}\n",
    "\n",
    "\t/* We set VM_ACCOUNT in a shared mapping's vm_flags, to inform\n",
    "\t * shmem_zero_setup (perhaps called through /dev/zero's ->mmap)\n",
    "\t * that memory reservation must be checked; but that reservation\n",
    "\t * belongs to shared memory object, not to vma: so now clear it.\n",
    "\t */\n",
    "\tif ((vm_flags & (VM_SHARED|VM_ACCOUNT)) == (VM_SHARED|VM_ACCOUNT))\n",
    "\t\tvma->vm_flags &= ~VM_ACCOUNT;\n",
    "\n",
    "\t/* Can addr have changed??\n",
    "\t *\n",
    "\t * Answer: Yes, several device drivers can do it in their\n",
    "\t *         f_op->mmap method. -DaveM\n",
    "\t */\n",
    "\taddr = vma->vm_start;\n",
    "\tpgoff = vma->vm_pgoff;\n",
    "\tvm_flags = vma->vm_flags;\n",
    "\n",
    "\tif (!file || !vma_merge(mm, prev, addr, vma->vm_end,\n",
    "\t\t\tvma->vm_flags, NULL, file, pgoff, vma_policy(vma))) {\n",
    "\t\tfile = vma->vm_file;\n",
    "\t\tvma_link(mm, vma, prev, rb_link, rb_parent);\n",
    "\t\tif (correct_wcount)\n",
    "\t\t\tatomic_inc(&inode->i_writecount);\n",
    "\t} else {\n",
    "\t\tif (file) {\n",
    "\t\t\tif (correct_wcount)\n",
    "\t\t\t\tatomic_inc(&inode->i_writecount);\n",
    "\t\t\tfput(file);\n",
    "\t\t}\n",
    "\t\tmpol_free(vma_policy(vma));\n",
    "\t\tkmem_cache_free(vm_area_cachep, vma);\n",
    "\t}\n",
    "out:\t\n",
    "\tmm->total_vm += len >> PAGE_SHIFT;\n",
    "\t__vm_stat_account(mm, vm_flags, file, len >> PAGE_SHIFT);\n",
    "\tif (vm_flags & VM_LOCKED) {\n",
    "\t\tmm->locked_vm += len >> PAGE_SHIFT;\n",
    "\t\tmake_pages_present(addr, addr + len);\n",
    "\t}\n",
    "\tif (flags & MAP_POPULATE) {\n",
    "\t\tup_write(&mm->mmap_sem);\n",
    "\t\tsys_remap_file_pages(addr, len, 0,\n",
    "\t\t\t\t\tpgoff, flags & MAP_NONBLOCK);\n",
    "\t\tdown_write(&mm->mmap_sem);\n",
    "\t}\n",
    "\tacct_update_integrals();\n",
    "\tupdate_mem_hiwater();\n",
    "\treturn addr;\n",
    "\n",
    "unmap_and_free_vma:\n",
    "\tif (correct_wcount)\n",
    "\t\tatomic_inc(&inode->i_writecount);\n",
    "\tvma->vm_file = NULL;\n",
    "\tfput(file);\n",
    "\n",
    "\t/* Undo any partial mapping done by a device driver. */\n",
    "\tzap_page_range(vma, vma->vm_start, vma->vm_end - vma->vm_start, NULL);\n",
    "free_vma:\n",
    "\tkmem_cache_free(vm_area_cachep, vma);\n",
    "unacct_error:\n",
    "\tif (charged)\n",
    "\t\tvm_unacct_memory(charged);\n",
    "\treturn error;\n",
    "}\n",
    "\n",
    "EXPORT_SYMBOL(do_mmap_pgoff);\n",
    "```\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heap\n",
    "\n",
    "![heap01](resources/heap01.png)\n",
    "![heap02](resources/hep02.png)\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linux2.6/mm/mmap.c\n",
    "\n",
    "```c\n",
    "/*\n",
    " *  this is really a simplified \"do_mmap\".  it only handles\n",
    " *  anonymous maps.  eventually we may be able to do some\n",
    " *  brk-specific accounting here.\n",
    " */\n",
    "unsigned long do_brk(unsigned long addr, unsigned long len)\n",
    "{\n",
    "\tstruct mm_struct * mm = current->mm;\n",
    "\tstruct vm_area_struct * vma, * prev;\n",
    "\tunsigned long flags;\n",
    "\tstruct rb_node ** rb_link, * rb_parent;\n",
    "\tpgoff_t pgoff = addr >> PAGE_SHIFT;\n",
    "\n",
    "\tlen = PAGE_ALIGN(len);\n",
    "\tif (!len)\n",
    "\t\treturn addr;\n",
    "\n",
    "\tif ((addr + len) > TASK_SIZE || (addr + len) < addr)\n",
    "\t\treturn -EINVAL;\n",
    "\n",
    "\t/*\n",
    "\t * mlock MCL_FUTURE?\n",
    "\t */\n",
    "\tif (mm->def_flags & VM_LOCKED) {\n",
    "\t\tunsigned long locked, lock_limit;\n",
    "\t\tlocked = mm->locked_vm << PAGE_SHIFT;\n",
    "\t\tlock_limit = current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur;\n",
    "\t\tlocked += len;\n",
    "\t\tif (locked > lock_limit && !capable(CAP_IPC_LOCK))\n",
    "\t\t\treturn -EAGAIN;\n",
    "\t}\n",
    "\n",
    "\t/*\n",
    "\t * mm->mmap_sem is required to protect against another thread\n",
    "\t * changing the mappings in case we sleep.\n",
    "\t */\n",
    "\tverify_mm_writelocked(mm);\n",
    "\n",
    "\t/*\n",
    "\t * Clear old maps.  this also does some error checking for us\n",
    "\t */\n",
    " munmap_back:\n",
    "    /*\n",
    "    如果分配的地址处有其他的vma，则释放。\n",
    "\tbrk函数给的地址可以增加，也可以减少, 减少就是释放\n",
    "\t内核并不知道某段虚拟内存程序到底用不用了\n",
    "\t所以只要程序新分配内存跟之前分配的内存有overlap，就会把之前的释放\n",
    "\t程序自己需要保证别释放错了\n",
    "    */\n",
    "\tvma = find_vma_prepare(mm, addr, &prev, &rb_link, &rb_parent);\n",
    "\tif (vma && vma->vm_start < addr + len) {\n",
    "\t\tif (do_munmap(mm, addr, len))\n",
    "\t\t\treturn -ENOMEM;\n",
    "\t\tgoto munmap_back;\n",
    "\t}\n",
    "\n",
    "\t/* Check against address space limits *after* clearing old maps... */\n",
    "\tif ((mm->total_vm << PAGE_SHIFT) + len\n",
    "\t    > current->signal->rlim[RLIMIT_AS].rlim_cur)\n",
    "\t\treturn -ENOMEM;\n",
    "\n",
    "\tif (mm->map_count > sysctl_max_map_count)\n",
    "\t\treturn -ENOMEM;\n",
    "\n",
    "\tif (security_vm_enough_memory(len >> PAGE_SHIFT))\n",
    "\t\treturn -ENOMEM;\n",
    "\n",
    "\tflags = VM_DATA_DEFAULT_FLAGS | VM_ACCOUNT | mm->def_flags;\n",
    "\n",
    "\t/* Can we just expand an old private anonymous mapping? */\n",
    "\tif (vma_merge(mm, prev, addr, addr + len, flags,\n",
    "\t\t\t\t\tNULL, NULL, pgoff, NULL))\n",
    "\t\tgoto out;\n",
    "\n",
    "\t/*\n",
    "\t * create a vma struct for an anonymous mapping\n",
    "\t */\n",
    "    /*\n",
    "    所有的vma结构都是存储在内核空间\n",
    "    */\n",
    "\tvma = kmem_cache_alloc(vm_area_cachep, SLAB_KERNEL);\n",
    "\tif (!vma) {\n",
    "\t\tvm_unacct_memory(len >> PAGE_SHIFT);\n",
    "\t\treturn -ENOMEM;\n",
    "\t}\n",
    "\tmemset(vma, 0, sizeof(*vma));\n",
    "\n",
    "\tvma->vm_mm = mm;\n",
    "\tvma->vm_start = addr;\n",
    "\tvma->vm_end = addr + len;\n",
    "\tvma->vm_pgoff = pgoff;\n",
    "\tvma->vm_flags = flags;\n",
    "\tvma->vm_page_prot = protection_map[flags & 0x0f];\n",
    "\tvma_link(mm, vma, prev, rb_link, rb_parent);\n",
    "out:\n",
    "\tmm->total_vm += len >> PAGE_SHIFT;\n",
    "    /*\n",
    "    对于VM_LOCKED的标记，必须要实际分配物理内存。LOCKED的意思是这部分内存不会被swap出去\n",
    "    */\n",
    "\tif (flags & VM_LOCKED) {\n",
    "\t\tmm->locked_vm += len >> PAGE_SHIFT;\n",
    "\t\tmake_pages_present(addr, addr + len);\n",
    "\t}\n",
    "\tacct_update_integrals();\n",
    "\tupdate_mem_hiwater();\n",
    "\treturn addr;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "1. see comments in codes\n",
    "\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copy_from_user/copy_to_user/get_user/put_user\n",
    "\n",
    "这几个函数是用来内核与用户空间数据拷贝用的。在x86体系下，每个用户态的进程都映射了内核空间（低1G），而当用户进程进入内核态的时候，其页表是不需要切换的。所以内核态的函数拿的用户空间地址是可以直接访问的。但是为啥还需要这几个函数呢？\n",
    "\n",
    "1. 安全性。这几个函数会做address的check，保证access的的权限是正确的，地址范围是正确的，比如不能修改内核的东西。memcopy没有\n",
    "\n",
    "2. 处理page fault。因为用户空间可能会有些地址的page并没有分配而触发page fault。这几个函数handle的page fault的情况\n",
    "\n",
    "[copy_{to,from}_user()的思考](http://www.wowotech.net/memory_management/454.html)\n",
    "\n",
    "[User space memory access from the Linux kernel](https://developer.ibm.com/articles/l-kernel-memory-access/)\n",
    "\n",
    "[kernel level exception handling in Linux](https://www.kernel.org/doc/Documentation/x86/exception-tables.txt)\n",
    "\n",
    "[kernel level exception handling in Linux](../../refs/kernel_level_exception_handling_in_linux.txt)\n",
    "\n",
    "\n",
    "![addresschecking01](resources/addresschecking01.png)\n",
    "![addresschecking01](resources/addresschecking02.png)\n",
    "\n",
    "----------------------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1be8c8bdc6fd86eebde1498154837c0fcc29bda10079a062db8bdccc6e4e2b17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
