{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some concepts\n",
    "\n",
    "* stack canary\n",
    "\n",
    "Stack canaries, named for their analogy to a canary in a coal mine, are used to detect a stack buffer overflow before execution of malicious code can occur. This method works by placing a small integer, the value of which is randomly chosen at program start, in memory just before the stack return pointer. Most buffer overflows overwrite memory from lower to higher memory addresses, so in order to overwrite the return pointer (and thus take control of the process) the canary value must also be overwritten. This value is checked to make sure it has not changed before a routine uses the return pointer on the stack.[2] This technique can greatly increase the difficulty of exploiting a stack buffer overflow because it forces the attacker to gain control of the instruction pointer by some non-traditional means such as corrupting other important variables on the stack.[2]\n",
    "\n",
    "----------------------------\n",
    "\n",
    "* PIC (programmable interrupt controller)\n",
    "\n",
    "In the old machines there was a PIC which is a chip responsible for sequentially processing multiple interrupt requests from multiple devices. In the new machines there is an Advanced Programmable Interrupt Controller commonly known as - APIC. An APIC consists of two separate devices:\n",
    "\n",
    "Local APIC\n",
    "I/O APIC\n",
    "\n",
    "The first - Local APIC is located on each CPU core. The local APIC is responsible for handling the CPU-specific interrupt configuration. The local APIC is usually used to manage interrupts from the APIC-timer, thermal sensor and any other such locally connected I/O devices.\n",
    "\n",
    "The second - I/O APIC provides multi-processor interrupt management. It is used to distribute external interrupts among the CPU cores. \n",
    "\n",
    "---------------------------\n",
    "\n",
    "* IDT (interrupt descriptor table)\n",
    "\n",
    "Addresses of each of the interrupt handlers are maintained in a special location referred to as the - Interrupt Descriptor Table or IDT\n",
    "\n",
    "he first 32 vector numbers from 0 to 31 are reserved by the processor and used for the processing of architecture-defined exceptions and interrupts. You can find the table with the description of these vector numbers in the second part of the Linux kernel initialization process - Early interrupt and exception handling. Vector numbers from 32 to 255 are designated as user-defined interrupts and are not reserved by the processor.\n",
    "\n",
    " the IDT entries are called gates. It can contain one of the following gates:\n",
    "\n",
    "Interrupt gates\n",
    "\n",
    "Task gates\n",
    "\n",
    "Trap gates.\n",
    "\n",
    "\n",
    "IDT entry struct\n",
    "\n",
    "```\n",
    "127                                                                             96\n",
    "+-------------------------------------------------------------------------------+\n",
    "|                                                                               |\n",
    "|                                Reserved                                       |\n",
    "|                                                                               |\n",
    "+--------------------------------------------------------------------------------\n",
    "95                                                                              64\n",
    "+-------------------------------------------------------------------------------+\n",
    "|                                                                               |\n",
    "|                               Offset 63..32                                   |\n",
    "|                                                                               |\n",
    "+-------------------------------------------------------------------------------+\n",
    "63                               48 47      46  44   42    39             34    32\n",
    "+-------------------------------------------------------------------------------+\n",
    "|                                  |       |  D  |   |     |      |   |   |     |\n",
    "|       Offset 31..16              |   P   |  P  | 0 |Type |0 0 0 | 0 | 0 | IST |\n",
    "|                                  |       |  L  |   |     |      |   |   |     |\n",
    " -------------------------------------------------------------------------------+\n",
    "31                                   16 15                                      0\n",
    "+-------------------------------------------------------------------------------+\n",
    "|                                      |                                        |\n",
    "|          Segment Selector            |                 Offset 15..0           |\n",
    "|                                      |                                        |\n",
    "+-------------------------------------------------------------------------------+\n",
    "```\n",
    "\n",
    "```c\n",
    "/* 16byte gate */\n",
    "struct gate_struct64 {\n",
    "\tu16 offset_low;\n",
    "\tu16 segment;\n",
    "\tunsigned ist : 3, zero0 : 5, type : 5, dpl : 2, p : 1;\n",
    "\tu16 offset_middle;\n",
    "\tu32 offset_high;\n",
    "\tu32 zero1;\n",
    "} __attribute__((packed));\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "* software interrupt\n",
    "\n",
    "Faults / Traps / Aborts\n",
    "\n",
    "A fault is an exception reported before the execution of a \"faulty\" instruction (which can then be corrected). If correct, it allows the interrupted program to resume.\n",
    "\n",
    "Next a trap is an exception, which is reported immediately following the execution of the trap instruction. Traps also allow the interrupted program to be continued just as a fault does.\n",
    "\n",
    "Finally, an abort is an exception that does not always report the exact instruction which caused the exception and does not allow the interrupted program to be resumed.\n",
    "\n",
    "--------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stack size\n",
    "\n",
    "### arch/x86/include/asm/page_64_types.h\n",
    "\n",
    "```c\n",
    "#define THREAD_SIZE_ORDER\t(2 + KASAN_STACK_ORDER)\n",
    "#define THREAD_SIZE  (PAGE_SIZE << THREAD_SIZE_ORDER)\n",
    "#define CURRENT_MASK (~(THREAD_SIZE - 1))\n",
    "\n",
    "#define EXCEPTION_STACK_ORDER (0 + KASAN_STACK_ORDER)\n",
    "#define EXCEPTION_STKSZ (PAGE_SIZE << EXCEPTION_STACK_ORDER)\n",
    "\n",
    "#define DEBUG_STACK_ORDER (EXCEPTION_STACK_ORDER + 1)\n",
    "#define DEBUG_STKSZ (PAGE_SIZE << DEBUG_STACK_ORDER)\n",
    "\n",
    "#define IRQ_STACK_ORDER (2 + KASAN_STACK_ORDER)\n",
    "#define IRQ_STACK_SIZE (PAGE_SIZE << IRQ_STACK_ORDER)\n",
    "\n",
    "#define DOUBLEFAULT_STACK 1\n",
    "#define NMI_STACK 2\n",
    "#define DEBUG_STACK 3\n",
    "#define MCE_STACK 4\n",
    "#define N_EXCEPTION_STACKS 4  /* hw limit: 7 */\n",
    "```\n",
    "\n",
    "1. each active thread stack size is `THREAD_SIZE`. `KASAN` is a runtime memory debugger.\n",
    "\n",
    "2. There also exist specialized stacks that are associated with each available CPU. These stacks are active when the kernel is executing on that CPU. When the user-space is executing on the CPU, these stacks do not contain any useful information. Each CPU has a few special per-cpu stacks as well. \n",
    "\n",
    "The first is `interrupt stack` used for the external hardware interrupts. Its size is `IRQ_STASCK_SIZE`. It's defined in `arch/x86/include/asm/processor.h`\n",
    "\n",
    "```c\n",
    "\n",
    "/*\n",
    " * Save the original ist values for checking stack pointers during debugging\n",
    " */\n",
    "struct orig_ist {\n",
    "\tunsigned long\t\tist[7];\n",
    "};\n",
    "\n",
    "#ifdef CONFIG_X86_64\n",
    "DECLARE_PER_CPU(struct orig_ist, orig_ist);\n",
    "\n",
    "union irq_stack_union {\n",
    "\tchar irq_stack[IRQ_STACK_SIZE];\n",
    "\t/*\n",
    "\t * GCC hardcodes the stack canary as %gs:40.  Since the\n",
    "\t * irq_stack is the object at %gs:0, we reserve the bottom\n",
    "\t * 48 bytes of the irq stack for the canary.\n",
    "\t */\n",
    "\tstruct {\n",
    "\t\tchar gs_base[40];\n",
    "\t\tunsigned long stack_canary;\n",
    "\t};\n",
    "};\n",
    "\n",
    "DECLARE_PER_CPU_FIRST(union irq_stack_union, irq_stack_union) __visible;\n",
    "DECLARE_INIT_PER_CPU(irq_stack_union);\n",
    "\n",
    "DECLARE_PER_CPU(char *, irq_stack_ptr);\n",
    "DECLARE_PER_CPU(unsigned int, irq_count);\n",
    "```\n",
    "\n",
    "`irq_stack_union` is used for store per-cpu stack. On the x86_64, the `gs` register is shared by per-cpu area and stack canary (more about per-cpu variables you can read in the special part). All per-cpu symbols are zero-based and the gs points to the base of the per-cpu area.\n",
    "\n",
    "Note that gs_base is a 40 bytes array. GCC requires that stack canary will be on the fixed offset from the base of the gs and its value must be 40 for the x86_64 and 20 for the x86.\n",
    "\n",
    "\n",
    "3.  x86_64 has a feature called Interrupt Stack Table or IST and this feature provides the ability to switch to a new stack for events non-maskable interrupt, double fault etc. There can be up to seven IST entries per-cpu. Some of them are:\n",
    "\n",
    "DOUBLEFAULT_STACK\n",
    "\n",
    "NMI_STACK\n",
    "\n",
    "DEBUG_STACK\n",
    "\n",
    "MCE_STACK\n",
    "\n",
    "```c\n",
    "#define DOUBLEFAULT_STACK 1\n",
    "#define NMI_STACK 2\n",
    "#define DEBUG_STACK 3\n",
    "#define MCE_STACK 4\n",
    "```\n",
    "\n",
    "---------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [kernel stack on x86_64](https://github.com/torvalds/linux/blob/16f73eb02d7e1765ccab3d2018e0bd98eb93d973/Documentation/x86/kernel-stacks)\n",
    "\n",
    "x86_64 page size (PAGE_SIZE) is 4K.\n",
    "\n",
    "Like all other architectures, x86_64 has a kernel stack for every\n",
    "active thread.  These thread stacks are THREAD_SIZE (2*PAGE_SIZE) big.\n",
    "These stacks contain useful data as long as a thread is alive or a\n",
    "zombie. While the thread is in user space the kernel stack is empty\n",
    "except for the **thread_info** structure at the bottom.\n",
    "\n",
    "In addition to the per thread stacks, there are specialized stacks\n",
    "associated with each CPU.  These stacks are only used while the kernel\n",
    "is in control on that CPU; when a CPU returns to user space the\n",
    "specialized stacks contain no useful data.  The main CPU stacks are:\n",
    "\n",
    "* Interrupt stack.  IRQ_STACK_SIZE\n",
    "\n",
    "  Used for external hardware interrupts.  If this is the first external\n",
    "  hardware interrupt (i.e. not a nested hardware interrupt) then the\n",
    "  kernel switches from the current task to the interrupt stack.  Like\n",
    "  the split thread and interrupt stacks on i386, this gives more room\n",
    "  for kernel interrupt processing without having to increase the size\n",
    "  of every per thread stack.\n",
    "\n",
    "  The interrupt stack is also used when processing a softirq.\n",
    "\n",
    "Switching to the kernel interrupt stack is done by software based on a\n",
    "per CPU interrupt nest counter. This is needed because x86-64 \"IST\"\n",
    "hardware stacks cannot nest without races.\n",
    "\n",
    "x86_64 also has a feature which is not available on i386, the ability\n",
    "to automatically switch to a new stack for designated events such as\n",
    "double fault or NMI, which makes it easier to handle these **unusual**\n",
    "events on x86_64.  This feature is called the Interrupt Stack Table\n",
    "(IST).  There can be up to 7 IST entries per CPU. The IST code is an\n",
    "index into the Task State Segment (TSS). The IST entries in the TSS\n",
    "point to dedicated stacks; each stack can be a different size.\n",
    "\n",
    "An IST is selected by a non-zero value in the IST field of an\n",
    "interrupt-gate descriptor. When an interrupt occurs and the hardware\n",
    "loads such a descriptor, the hardware automatically sets the new stack\n",
    "pointer based on the IST value, then invokes the interrupt handler.  **If\n",
    "the interrupt came from user mode, then the interrupt handler prologue\n",
    "will switch back to the per-thread stack. If software wants to allow\n",
    "nested IST interrupts then the handler must adjust the IST values on\n",
    "entry to and exit from the interrupt handler.**  (This is occasionally\n",
    "done, e.g. for debug exceptions.)\n",
    "\n",
    "Events with different IST codes (i.e. with different stacks) can be\n",
    "nested.  For example, a debug interrupt can safely be interrupted by an\n",
    "NMI.  arch/x86_64/kernel/entry.S::paranoidentry adjusts the stack\n",
    "pointers on entry to and exit from all IST events, in theory allowing\n",
    "IST events with the same code to be nested.  However in most cases, the\n",
    "stack size allocated to an IST assumes no nesting for the same code.\n",
    "If that assumption is ever broken then the stacks will become corrupt.\n",
    "\n",
    "The currently assigned IST stacks are :-\n",
    "\n",
    "* DOUBLEFAULT_STACK.  EXCEPTION_STKSZ (PAGE_SIZE).\n",
    "\n",
    "  Used for interrupt 8 - Double Fault Exception (#DF).\n",
    "\n",
    "  Invoked when handling one exception causes another exception. Happens\n",
    "  when the kernel is very confused (e.g. kernel stack pointer corrupt).\n",
    "  **Using a separate stack allows the kernel to recover from it well enough\n",
    "  in many cases to still output an oops.**\n",
    "\n",
    "* NMI_STACK.  EXCEPTION_STKSZ (PAGE_SIZE).\n",
    "\n",
    "  Used for non-maskable interrupts (NMI).\n",
    "\n",
    "  NMI can be delivered at any time, including when the kernel is in the\n",
    "  middle of switching stacks.  Using IST for NMI events avoids making\n",
    "  assumptions about the previous state of the kernel stack.\n",
    "\n",
    "* DEBUG_STACK.  DEBUG_STKSZ\n",
    "\n",
    "  Used for hardware debug interrupts (interrupt 1) and for software\n",
    "  debug interrupts (INT3).\n",
    "\n",
    "  When debugging a kernel, debug interrupts (both hardware and\n",
    "  software) can occur at any time.  Using IST for these interrupts\n",
    "  avoids making assumptions about the previous state of the kernel\n",
    "  stack.\n",
    "\n",
    "* MCE_STACK.  EXCEPTION_STKSZ (PAGE_SIZE).\n",
    "\n",
    "  Used for interrupt 18 - Machine Check Exception (#MC).\n",
    "\n",
    "  MCE can be delivered at any time, including when the kernel is in the\n",
    "  middle of switching stacks.  Using IST for MCE events avoids making\n",
    "  assumptions about the previous state of the kernel stack.\n",
    "\n",
    "For more details see the Intel IA32 or AMD AMD64 architecture manuals.\n",
    "\n",
    "---------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug and Breakpoint exceptions\n",
    "\n",
    "**#DB or debug exception** occurs when a debug event occurs. For example - attempt to change the contents of a debug register. Debug registers are special registers that were presented in x86 processors starting from the Intel 80386 processor and as you can understand from name of this CPU extension, main purpose of these registers is debugging.\n",
    "\n",
    "These registers allow to set breakpoints on the code and read or write data to trace it. Debug registers may be accessed only in the privileged mode and an attempt to read or write the debug registers when executing at any other privilege level causes a general protection fault exception. That's why we have used set_intr_gate_ist for the #DB exception, but not the set_system_intr_gate_ist.\n",
    "\n",
    "\n",
    "**#BP or breakpoint exception** occurs when processor executes the int 3 instruction. Unlike the DB exception, the #BP exception may occur in userspace. We can add it anywhere in our code, for example let's look on the simple program:\n",
    "\n",
    "```c\n",
    "// breakpoint.c\n",
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    int i;\n",
    "    while (i < 6){\n",
    "        printf(\"i equal to: %d\\n\", i);\n",
    "        __asm__(\"int3\");\n",
    "        ++i;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "run\n",
    "\n",
    "```bash\n",
    "$ gcc breakpoint.c -o breakpoint\n",
    "i equal to: 0\n",
    "Trace/breakpoint trap\n",
    "```\n",
    "\n",
    "\n",
    "using GDB\n",
    "\n",
    "```bash\n",
    "$ gdb breakpoint\n",
    "...\n",
    "...\n",
    "...\n",
    "(gdb) run\n",
    "Starting program: /home/alex/breakpoints \n",
    "i equal to: 0\n",
    "\n",
    "Program received signal SIGTRAP, Trace/breakpoint trap.\n",
    "0x0000000000400585 in main ()\n",
    "=> 0x0000000000400585 <main+31>:    83 45 fc 01    add    DWORD PTR [rbp-0x4],0x1\n",
    "(gdb) c\n",
    "Continuing.\n",
    "i equal to: 1\n",
    "\n",
    "Program received signal SIGTRAP, Trace/breakpoint trap.\n",
    "0x0000000000400585 in main ()\n",
    "=> 0x0000000000400585 <main+31>:    83 45 fc 01    add    DWORD PTR [rbp-0x4],0x1\n",
    "(gdb) c\n",
    "Continuing.\n",
    "i equal to: 2\n",
    "\n",
    "Program received signal SIGTRAP, Trace/breakpoint trap.\n",
    "0x0000000000400585 in main ()\n",
    "=> 0x0000000000400585 <main+31>:    83 45 fc 01    add    DWORD PTR [rbp-0x4],0x1\n",
    "...\n",
    "...\n",
    "...\n",
    "```\n",
    "---------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exception handler\n",
    "\n",
    "### arch/x86/include/asm/traps.h\n",
    "\n",
    "```c\n",
    "asmlinkage void divide_error(void);\n",
    "asmlinkage void debug(void);\n",
    "asmlinkage void nmi(void);\n",
    "asmlinkage void int3(void);\n",
    "asmlinkage void xen_debug(void);\n",
    "asmlinkage void xen_int3(void);\n",
    "asmlinkage void xen_stack_segment(void);\n",
    "asmlinkage void overflow(void);\n",
    "asmlinkage void bounds(void);\n",
    "asmlinkage void invalid_op(void);\n",
    "asmlinkage void device_not_available(void);\n",
    "```\n",
    "\n",
    "1. declare some traps exception handler\n",
    "\n",
    "2. `asmlinkage` directive is the special specificator of GCC. Actually for a C functions which are called from assembly, we need in explicit declaration of the function calling convention. In our case, if function made with asmlinkage descriptor, then gcc will compile the function to retrieve parameters from stack.\n",
    "\n",
    "----------------\n",
    "\n",
    "### arch/x86/entry/entry_64.S\n",
    "\n",
    "```assembly\n",
    ".macro idtentry sym do_sym has_error_code:req paranoid=0 shift_ist=-1\n",
    "ENTRY(\\sym)\n",
    "\t/* Sanity check */\n",
    "\t.if \\shift_ist != -1 && \\paranoid == 0\n",
    "\t.error \"using shift_ist requires paranoid=1\"\n",
    "\t.endif\n",
    "....\n",
    "```\n",
    "\n",
    "```assembly\n",
    "idtentry debug\t\t\tdo_debug\t\thas_error_code=0\tparanoid=1 shift_ist=DEBUG_STACK\n",
    "idtentry int3\t\t\tdo_int3\t\t\thas_error_code=0\tparanoid=1 shift_ist=DEBUG_STACK\n",
    "```\n",
    "\n",
    "* sym - defines global symbol with the .globl name which will be an an entry of exception handler;\n",
    "\n",
    "* do_sym - symbol name which represents a secondary entry of an exception handler;\n",
    "\n",
    "* has_error_code - information about existence of an error code of exception.\n",
    "\n",
    "* paranoid - shows us how we need to check current mode;\n",
    "\n",
    "* shift_ist - shows us is an exception running at Interrupt Stack Table\n",
    "\n",
    "\n",
    "1. **paranoid** defines the method which helps us to know did we come from userspace or not to an exception handler. The easiest way to determine this is to via CPL or Current Privilege Level in CS segment register. If it is equal to 3, we came from userspace, if zero we came from kernel space:\n",
    "\n",
    "![paranoid](resources/paranoid.png)\n",
    "\n",
    "------------------\n",
    "\n",
    "Each exception handler may be consists from two parts. \n",
    "\n",
    "The first part is generic part and it is the same for all exception handlers. An exception handler should to save general purpose registers on the stack, switch to kernel stack if an exception came from userspace and transfer control to the second part of an exception handler. \n",
    "\n",
    "The second part of an exception handler does certain work depends on certain exception. For example page fault exception handler should find virtual page for given address, invalid opcode exception handler should send SIGILL signal and etc.\n",
    "\n",
    "As we may read in the [Intel® 64 and IA-32 Architectures Software Developer’s Manual 3A](https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html), the state of stack when an exception occurs is following:\n",
    "\n",
    "```\n",
    "    +------------+\n",
    "+40 | %SS        |\n",
    "+32 | %RSP       |\n",
    "+24 | %RFLAGS    |\n",
    "+16 | %CS        |\n",
    " +8 | %RIP       |\n",
    "  0 | ERROR CODE | <-- %RSP\n",
    "    +------------+\n",
    "```\n",
    "\n",
    "So the SS(stack segment register), RSP(stack pointer register), RFLAGS(flag register), CS(code segment register), RIP(instruciton pointer register) are already stored on the stack\n",
    "\n",
    "After we pushed fake error code on the stack, we should allocate space for general purpose registers with `ALLOC_PT_GPREGS_ON_STACK`\n",
    "\n",
    "### arch/x86/entry/calling.h\n",
    "\n",
    "```\n",
    "\t.macro ALLOC_PT_GPREGS_ON_STACK addskip=0\n",
    "\taddq\t$-(15*8+\\addskip), %rsp\n",
    "\t.endm\n",
    "```\n",
    "\n",
    "So the stack will be \n",
    "\n",
    "```\n",
    "     +------------+\n",
    "+160 | %SS        |\n",
    "+152 | %RSP       |\n",
    "+144 | %RFLAGS    |\n",
    "+136 | %CS        |\n",
    "+128 | %RIP       |\n",
    "+120 | ERROR CODE |\n",
    "     |------------|\n",
    "+112 |            |\n",
    "+104 |            |\n",
    " +96 |            |\n",
    " +88 |            |\n",
    " +80 |            |\n",
    " +72 |            |\n",
    " +64 |            |\n",
    " +56 |            |\n",
    " +48 |            |\n",
    " +40 |            |\n",
    " +32 |            |\n",
    " +24 |            |\n",
    " +16 |            |\n",
    "  +8 |            |\n",
    "  +0 |            | <- %RSP\n",
    "     +------------+\n",
    "```\n",
    "\n",
    "\n",
    "* An exception occured in userspace\n",
    "\n",
    "Firstly we save all general purpose registers in the previously allocated area on the stack\n",
    "\n",
    "```\n",
    "SAVE_C_REGS 8\n",
    "SAVE_EXTRA_REGS 8\n",
    "```\n",
    "\n",
    "```c\n",
    ".macro SAVE_EXTRA_REGS offset=0\n",
    "    movq %r15, 0*8+\\offset(%rsp)\n",
    "    movq %r14, 1*8+\\offset(%rsp)\n",
    "    movq %r13, 2*8+\\offset(%rsp)\n",
    "    movq %r12, 3*8+\\offset(%rsp)\n",
    "    movq %rbp, 4*8+\\offset(%rsp)\n",
    "    movq %rbx, 5*8+\\offset(%rsp)\n",
    ".endm\n",
    "```\n",
    "\n",
    "After calling `SAVE_C_REGS` and `SAVE_EXTRA_REGS`, the stack will be\n",
    "\n",
    "```\n",
    "     +------------+\n",
    "+160 | %SS        |\n",
    "+152 | %RSP       |\n",
    "+144 | %RFLAGS    |\n",
    "+136 | %CS        |\n",
    "+128 | %RIP       |\n",
    "+120 | ERROR CODE |\n",
    "     |------------|\n",
    "+112 | %RDI       |\n",
    "+104 | %RSI       |\n",
    " +96 | %RDX       |\n",
    " +88 | %RCX       |\n",
    " +80 | %RAX       |\n",
    " +72 | %R8        |\n",
    " +64 | %R9        |\n",
    " +56 | %R10       |\n",
    " +48 | %R11       |\n",
    " +40 | %RBX       |\n",
    " +32 | %RBP       |\n",
    " +24 | %R12       |\n",
    " +16 | %R13       |\n",
    "  +8 | %R14       |\n",
    "  +0 | %R15       | <- %RSP\n",
    "     +------------+\n",
    "```\n",
    "\n",
    "`SWAPGS` instruction will be executed and values from `MSR_KERNEL_GS_BASE` and `MSR_GS_BASE` will be swapped. From this moment the `%gs` register will point to the base address of kernel structures. So, the SWAPGS instruction is called and it was main point of the error_entry routing\n",
    "\n",
    "\n",
    "Then\n",
    "\n",
    "```\n",
    "movq    %rsp, %rdi\n",
    "call    sync_regs\n",
    "```\n",
    "\n",
    "Here we put base address of stack pointer `%rdi` register which will be first argument (according to x86_64 ABI) of the sync_regs function and call this function which is defined in the `arch/x86/kernel/traps.c` source code file:\n",
    "\n",
    "```c\n",
    "asmlinkage __visible notrace struct pt_regs *sync_regs(struct pt_regs *eregs)\n",
    "{\n",
    "    struct pt_regs *regs = task_pt_regs(current);\n",
    "    *regs = *eregs;\n",
    "    return regs;\n",
    "}\n",
    "```\n",
    "\n",
    "in `arch/x86/asm/processor.h`\n",
    "```c\n",
    "#define task_pt_regs(tsk)       ((struct pt_regs *)(tsk)->thread.sp0 - 1)\n",
    "```\n",
    "\n",
    "1. `thread.sp0` is the normal kernel stack position\n",
    "\n",
    "2. `sync_regs` copy the registers contents from userspace stack to kernel stack. the `pt_regs` is in `arch/x86/include/uapi/asm/ptrace.h`\n",
    "\n",
    "```c\n",
    "struct pt_regs {\n",
    "/*\n",
    " * C ABI says these regs are callee-preserved. They aren't saved on kernel entry\n",
    " * unless syscall needs a complete, fully filled \"struct pt_regs\".\n",
    " */\n",
    "\tunsigned long r15;\n",
    "\tunsigned long r14;\n",
    "\tunsigned long r13;\n",
    "\tunsigned long r12;\n",
    "\tunsigned long rbp;\n",
    "\tunsigned long rbx;\n",
    "/* These regs are callee-clobbered. Always saved on kernel entry. */\n",
    "\tunsigned long r11;\n",
    "\tunsigned long r10;\n",
    "\tunsigned long r9;\n",
    "\tunsigned long r8;\n",
    "\tunsigned long rax;\n",
    "\tunsigned long rcx;\n",
    "\tunsigned long rdx;\n",
    "\tunsigned long rsi;\n",
    "\tunsigned long rdi;\n",
    "/*\n",
    " * On syscall entry, this is syscall#. On CPU exception, this is error code.\n",
    " * On hw interrupt, it's IRQ number:\n",
    " */\n",
    "\tunsigned long orig_rax;\n",
    "/* Return frame for iretq */\n",
    "\tunsigned long rip;\n",
    "\tunsigned long cs;\n",
    "\tunsigned long eflags;\n",
    "\tunsigned long rsp;\n",
    "\tunsigned long ss;\n",
    "/* top of stack page */\n",
    "};\n",
    "```\n",
    "\n",
    "After copying the register to kernel stack, we switch the stack by\n",
    "\n",
    "```c\n",
    "movq    %rax, %rsp\n",
    "```\n",
    "\n",
    "`%rax` has the return value of `sync_regs`, which is the kernel stack position.\n",
    "\n",
    "Finally `call \\do_sym`\n",
    "\n",
    "After secondary handler will finish its works, we will return to the idtentry macro and the next step will be jump to the error_exit:\n",
    "\n",
    "```\n",
    "jmp    error_exit\n",
    "```\n",
    "\n",
    "routine. The `error_exit` function defined in the same `arch/x86/entry/entry_64.S` assembly source code file and the main goal of this function is to know where we are from (from userspace or kernelspace) and execute `SWPAGS` depends on this. Restore registers to previous state and execute `iret` instruction to transfer control to an interrupted task.\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMI iret problem\n",
    "\n",
    "[x86 NMI iret problem](https://lwn.net/Articles/484932/)\n",
    "\n",
    "[solution](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=3f3c8b8c4b2a34776c3470142a7c8baafcda6eb0)\n",
    "\n",
    "1. X86的NMI，一旦触发，开始执行handler，后面如果再有新的NMI，会被block，一直等到第一个执行完了，才会执行第二个。而如果执行第一个的时候，后面有多个nmi，只有第二个会被queue，其他的都会被抛弃\n",
    "\n",
    "2. 触发NMI后，不会有新的NMI 中断当前handler，只有当`iret`指令被调用之后，新的NMI又会被触发\n",
    "\n",
    "3. 所以带来的问题就是，如果第一个NMI的handler里面有处理一些exception（如breakpoint、page fault）的handler，那么当这个exception handler返回的时候，就会调用`iret`，就会又把NMI打开了，导致nested NMI问题\n",
    "\n",
    "4. 在x86_64架构下，NMI有自己单独的stack，位置是固定的，一旦触发NMI中断，硬件会自动把当前的context 信息（SS/SP/EF/CS/RIP）push进入这个单独的stack里面。这个位置是固定的。所以如果发生nested NMI，就会导致上一个的NMI 的返回context的信息被破坏了\n",
    "\n",
    "5. kernel中的解决办法是，当第一次NMI调用的时候，会插入一个NMI的标记变量（NMI executing variable），来表征是不是nested 调用。然后会把硬件push的context信息复制两份，如下\n",
    "\n",
    "```\n",
    "\t  +-------------------------+\n",
    "\t  | original SS             |\n",
    "\t  | original Return RSP     |\n",
    "\t  | original RFLAGS         |\n",
    "\t  | original CS             |\n",
    "\t  | original RIP            |\n",
    "\t  +-------------------------+\n",
    "\t  | temp storage for rdx    |\n",
    "\t  +-------------------------+\n",
    "\t  | NMI executing variable  |\n",
    "\t  +-------------------------+\n",
    "\t  | Saved SS                |\n",
    "\t  | Saved Return RSP        |\n",
    "\t  | Saved RFLAGS            |\n",
    "\t  | Saved CS                |\n",
    "\t  | Saved RIP               |\n",
    "\t  +-------------------------+\n",
    "\t  | copied SS               |\n",
    "\t  | copied Return RSP       |\n",
    "\t  | copied RFLAGS           |\n",
    "\t  | copied CS               |\n",
    "\t  | copied RIP              |\n",
    "\t  +-------------------------+\n",
    "\t  | pt_regs                 |\n",
    "\t  +-------------------------+  <-----A\n",
    "```\n",
    "\n",
    "这时候的栈底（EBP）就在 A 处\n",
    "\n",
    "6. 当在处理第一个NMI的时候触发了第二个NMI，第二个NMI就会把上面origional部分给破坏掉，然后回去检查`NMI executing variable`，同时还要检查现在的stack是不是NMIstack，如果 variable 被设置了，并且当前的stack是NMI stack，那么就说明是nested NMI\n",
    "\n",
    "```\n",
    "A special location on the NMI stack holds a variable that is set when\n",
    "the first NMI handler runs. If this variable is set then we know that\n",
    "this is a nested NMI and we process the nested NMI code.\n",
    "\n",
    "There is still a race when this variable is cleared and an NMI comes\n",
    "in just before the first NMI does the return. For this case, if the\n",
    "variable is cleared, we also check if the interrupted stack is the\n",
    "NMI stack. If it is, then we process the nested NMI code.\n",
    "\n",
    "Why the two tests and not just test the interrupted stack?\n",
    "\n",
    "If the first NMI hits a breakpoint and loses NMI context, and then it\n",
    "hits another breakpoint and while processing that breakpoint we get a\n",
    "nested NMI. When processing a breakpoint, the stack changes to the\n",
    "breakpoint stack. If another NMI comes in here we can't rely on the\n",
    "interrupted stack to be the NMI stack.\n",
    "```\n",
    "\n",
    "7. 当时一个nested NMI的时候，就更改上面`copied`的部分，让其指向一个新的区域（fix up location）。当第一个NMI iret的时候，会进入fix up location，这部分会重新把`saved` 的重新部分拷贝到`copied` 区域，然后开始执行第二个NMI的handler。周而复始\n",
    "\n",
    "8. 如果第三个NMI被触发，会检查当前是不是在fix up location，如果是，就直接返回。跟x86硬件设计一样，最多两个NMI\n",
    "\n",
    "```\n",
    "NMI handler while it is in the fixup location. If it has, it will not\n",
    "modify the copied interrupt stack and will just leave as if nothing\n",
    "happened. As the NMI handle is about to execute again, there's no reason\n",
    "to latch now.\n",
    "```\n",
    "\n",
    "Yes, this is really hard part ...\n",
    "\n",
    "---------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## two parts of interrupts \n",
    "\n",
    "Interrupt: An interrupt is an event that alter the sequence of instruction executed by a processor in corresponding to electrical signal generated by HW circuit both inside & outside CPU.\n",
    "\n",
    "When any interrupt is generated it is handled by two halves.\n",
    "\n",
    "1) Top Halves\n",
    "\n",
    "2) Bottom Halves\n",
    "\n",
    "Top Halves: Top halves executes as soon as CPU receives the interrupt. In the Top Halves Context Interrupt and Scheduler are disabled. This part of the code only contain Critical Code. Execution Time of this code should be as short as possible because at this time Interrupt are disabled, we don't want to miss out other interrupt by generated by the devices.\n",
    "\n",
    "Bottom Halves: The Job of the Bottom half is used to run left over (deferred) work by the top halves. When this piece of code is being Executed **interrupt is Enabled and Scheduler is Disabled**. **Bottom Halves are scheduled by Softirqs & Tasklets to run deferred work**\n",
    "\n",
    "Note: The top halves code should be as short as possible or deterministic time and should not contain any blocking calls as well.\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## interrupts and exception\n",
    "\n",
    "![](resources/int01.png)\n",
    "![](resources/int02.png)\n",
    "![](resources/int03.png)\n",
    "\n",
    "\n",
    "## IDT\n",
    "\n",
    "![](resources/idt01.png)\n",
    "![](resources/idt02.png)\n",
    "\n",
    "\n",
    "## Hardware handling of Interrupt\n",
    "\n",
    "![](resouces/int04.png)\n",
    "\n",
    "\n",
    "\n",
    "## Initializing the interrupt descriptor table\n",
    "\n",
    "![](resources/int07.png)\n",
    "\n",
    "\n",
    "## Exception Handling\n",
    "\n",
    "![](resources/int08.png)\n",
    "\n",
    "\n",
    "\n",
    "## Interrupt Handling\n",
    "\n",
    "![](resources/int09.png)\n",
    "\n",
    "\n",
    "### [Interrupts and CPL](../../refs/interrupts_cpl.pdf)\n",
    "\n",
    "![](resources/cpl01.png)\n",
    "![](resources/cpl02.png)\n",
    "![](resources/cpl03.png)\n",
    "\n",
    "-----------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](resources/int10.png)\n",
    "\n",
    "-------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](resources/int11.png)\n",
    "\n",
    "1. In the Step 4, if we run the process in user mode(CPL=3), how we switch to a kernel interrupt handler which need to run in CPL=4 ?\n",
    "\n",
    "Actually, the DPL is the descriptor PL, it's used to control which level can call this handler, not the real handler running PL.\n",
    "\n",
    "![](resources/int12.png)\n",
    "\n",
    "----------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](resources/int13.png)\n",
    "\n",
    "--------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](resources/exception01.png)\n",
    "![](resources/exception02.png)\n",
    "\n",
    "## linux2.6/arch/i386/kernel/traps.c\n",
    "\n",
    "```c\n",
    "static void do_trap(int trapnr, int signr, char *str, int vm86,\n",
    "\t\t\t   struct pt_regs * regs, long error_code, siginfo_t *info)\n",
    "{\n",
    "\tif (regs->eflags & VM_MASK) {\n",
    "\t\tif (vm86)\n",
    "\t\t\tgoto vm86_trap;\n",
    "\t\tgoto trap_signal;\n",
    "\t}\n",
    "\n",
    "\tif (!(regs->xcs & 3))\n",
    "\t\tgoto kernel_trap;\n",
    "\n",
    "\ttrap_signal: {\n",
    "\t\tstruct task_struct *tsk = current;\n",
    "\t\ttsk->thread.error_code = error_code;\n",
    "\t\ttsk->thread.trap_no = trapnr;\n",
    "\t\tif (info)\n",
    "\t\t\tforce_sig_info(signr, info, tsk);\n",
    "\t\telse\n",
    "\t\t\tforce_sig(signr, tsk);\n",
    "\t\treturn;\n",
    "\t}\n",
    "\n",
    "\tkernel_trap: {\n",
    "\t\tif (!fixup_exception(regs))\n",
    "\t\t\tdie(str, regs, error_code);\n",
    "\t\treturn;\n",
    "\t}\n",
    "\n",
    "\tvm86_trap: {\n",
    "\t\tint ret = handle_vm86_trap((struct kernel_vm86_regs *) regs, error_code, trapnr);\n",
    "\t\tif (ret) goto trap_signal;\n",
    "\t\treturn;\n",
    "\t}\n",
    "}\n",
    "```\n",
    "\n",
    "1. `force_sig` send a signal which the process can't ignore. `linux2.6/kernel/signal.c`\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IO Interrupt\n",
    "\n",
    "![](resources/int14.png)\n",
    "![](resources/int15.png)\n",
    "\n",
    "-------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linux2.6/include/linux/irq.h\n",
    "\n",
    "```c\n",
    "/*\n",
    " * This is the \"IRQ descriptor\", which contains various information\n",
    " * about the irq, including what kind of hardware handling it has,\n",
    " * whether it is disabled etc etc.\n",
    " *\n",
    " * Pad this out to 32 bytes for cache and indexing reasons.\n",
    " */\n",
    "typedef struct irq_desc {\n",
    "\thw_irq_controller *handler;\n",
    "\tvoid *handler_data;\n",
    "\tstruct irqaction *action;\t/* IRQ action list */\n",
    "\tunsigned int status;\t\t/* IRQ status */\n",
    "\tunsigned int depth;\t\t/* nested irq disables */\n",
    "\tunsigned int irq_count;\t\t/* For detecting broken interrupts */\n",
    "\tunsigned int irqs_unhandled;\n",
    "\tspinlock_t lock;\n",
    "} ____cacheline_aligned irq_desc_t;\n",
    "\n",
    "```\n",
    "\n",
    "![](resources/int16.png)\n",
    "\n",
    "1. `depth` is the disables count. Only no disables, it can be executed.\n",
    "\n",
    "\n",
    "```c\n",
    "/*\n",
    " * Interrupt controller descriptor. This is all we need\n",
    " * to describe about the low-level hardware. \n",
    " */\n",
    "struct hw_interrupt_type {\n",
    "\tconst char * typename;\n",
    "\tunsigned int (*startup)(unsigned int irq);\n",
    "\tvoid (*shutdown)(unsigned int irq);\n",
    "\tvoid (*enable)(unsigned int irq);\n",
    "\tvoid (*disable)(unsigned int irq);\n",
    "\tvoid (*ack)(unsigned int irq);\n",
    "\tvoid (*end)(unsigned int irq);\n",
    "\tvoid (*set_affinity)(unsigned int irq, cpumask_t dest);\n",
    "};\n",
    "```\n",
    "\n",
    "1. PIC/APIC. `ack` can disable the IRQ hw to generate new interrupt.\n",
    "\n",
    "\n",
    "```c\n",
    "/*\n",
    " * IRQ line status.\n",
    " */\n",
    "#define IRQ_INPROGRESS\t1\t/* IRQ handler active - do not enter! */\n",
    "#define IRQ_DISABLED\t2\t/* IRQ disabled - do not enter! */\n",
    "#define IRQ_PENDING\t4\t/* IRQ pending - replay on enable */\n",
    "#define IRQ_REPLAY\t8\t/* IRQ has been replayed but not acked yet */\n",
    "#define IRQ_AUTODETECT\t16\t/* IRQ is being autodetected */\n",
    "#define IRQ_WAITING\t32\t/* IRQ not yet seen - for autodetection */\n",
    "#define IRQ_LEVEL\t64\t/* IRQ level triggered */\n",
    "#define IRQ_MASKED\t128\t/* IRQ masked - shouldn't be seen again */\n",
    "#define IRQ_PER_CPU\t256\t/* IRQ is per CPU */\n",
    "```\n",
    "\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linux2.6/kernel/irq/handler.c\n",
    "\n",
    "### data struct\n",
    "\n",
    "```c\n",
    "/*\n",
    " * Linux has a controller-independent interrupt architecture.\n",
    " * Every controller has a 'controller-template', that is used\n",
    " * by the main code to do the right thing. Each driver-visible\n",
    " * interrupt source is transparently wired to the apropriate\n",
    " * controller. Thus drivers need not be aware of the\n",
    " * interrupt-controller.\n",
    " *\n",
    " * The code is designed to be easily extended with new/different\n",
    " * interrupt controllers, without having to do assembly magic or\n",
    " * having to touch the generic code.\n",
    " *\n",
    " * Controller mappings for all interrupt sources:\n",
    " */\n",
    "irq_desc_t irq_desc[NR_IRQS] __cacheline_aligned = {\n",
    "\t[0 ... NR_IRQS-1] = {\n",
    "\t\t.handler = &no_irq_type,\n",
    "\t\t.lock = SPIN_LOCK_UNLOCKED\n",
    "\t}\n",
    "};\n",
    "\n",
    "/*\n",
    " * Generic 'no controller' code\n",
    " */\n",
    "static void end_none(unsigned int irq) { }\n",
    "static void enable_none(unsigned int irq) { }\n",
    "static void disable_none(unsigned int irq) { }\n",
    "static void shutdown_none(unsigned int irq) { }\n",
    "static unsigned int startup_none(unsigned int irq) { return 0; }\n",
    "\n",
    "static void ack_none(unsigned int irq)\n",
    "{\n",
    "\t/*\n",
    "\t * 'what should we do if we get a hw irq event on an illegal vector'.\n",
    "\t * each architecture has to answer this themself.\n",
    "\t */\n",
    "\tack_bad_irq(irq);\n",
    "}\n",
    "\n",
    "struct hw_interrupt_type no_irq_type = {\n",
    "\t.typename = \t\"none\",\n",
    "\t.startup = \tstartup_none,\n",
    "\t.shutdown = \tshutdown_none,\n",
    "\t.enable = \tenable_none,\n",
    "\t.disable = \tdisable_none,\n",
    "\t.ack = \t\tack_none,\n",
    "\t.end = \t\tend_none,\n",
    "\t.set_affinity = NULL\n",
    "};\n",
    "```\n",
    "\n",
    "1. global IRQ array initiation\n",
    "\n",
    "\n",
    "### handle_IRQ_event\n",
    "\n",
    "```c\n",
    "/*\n",
    " * Have got an event to handle:\n",
    " */\n",
    "fastcall int handle_IRQ_event(unsigned int irq, struct pt_regs *regs,\n",
    "\t\t\t\tstruct irqaction *action)\n",
    "{\n",
    "\tint ret, retval = 0, status = 0;\n",
    "\n",
    "\tif (!(action->flags & SA_INTERRUPT))\n",
    "\t\tlocal_irq_enable();\n",
    "\n",
    "\tdo {\n",
    "\t\tret = action->handler(irq, action->dev_id, regs);\n",
    "\t\tif (ret == IRQ_HANDLED)\n",
    "\t\t\tstatus |= action->flags;\n",
    "\t\tretval |= ret;\n",
    "\t\taction = action->next;\n",
    "\t} while (action);\n",
    "\n",
    "\tif (status & SA_SAMPLE_RANDOM)\n",
    "\t\tadd_interrupt_randomness(irq);\n",
    "\tlocal_irq_disable();\n",
    "\n",
    "\treturn retval;\n",
    "}\n",
    "```\n",
    "\n",
    "1. handle an IRQ event by executing the action handler one by one.\n",
    "\n",
    "2. `add_interrupt_randomness(irq);` this is used as a random source for the linux internal entropy pool.\n",
    "\n",
    "\n",
    "### __do_IRQ\n",
    "\n",
    "```c\n",
    "/*\n",
    " * do_IRQ handles all normal device IRQ's (the special\n",
    " * SMP cross-CPU interrupts have their own specific\n",
    " * handlers).\n",
    " */\n",
    "fastcall unsigned int __do_IRQ(unsigned int irq, struct pt_regs *regs)\n",
    "{\n",
    "\tirq_desc_t *desc = irq_desc + irq;\n",
    "\tstruct irqaction * action;\n",
    "\tunsigned int status;\n",
    "\n",
    "\tkstat_this_cpu.irqs[irq]++;\n",
    "\n",
    "\t/** by xitongsys\n",
    "\t * For IRQ_PER_CPU, no need lock, just run handle_IRQ_event and return\n",
    "\t**/\n",
    "\tif (desc->status & IRQ_PER_CPU) {\n",
    "\t\tirqreturn_t action_ret;\n",
    "\n",
    "\t\t/*\n",
    "\t\t * No locking required for CPU-local interrupts:\n",
    "\t\t */\n",
    "\t\tdesc->handler->ack(irq);\n",
    "\t\taction_ret = handle_IRQ_event(irq, regs, desc->action);\n",
    "\t\tif (!noirqdebug)\n",
    "\t\t\tnote_interrupt(irq, desc, action_ret);\n",
    "\t\tdesc->handler->end(irq);\n",
    "\t\treturn 1;\n",
    "\t}\n",
    "\n",
    "\tspin_lock(&desc->lock);\n",
    "\n",
    "\t/** xitongsys\n",
    "\t * \n",
    "\t * ack is used to disable the irq line\n",
    "\t **/\n",
    "\tdesc->handler->ack(irq);\n",
    "\t/*\n",
    "\t * REPLAY is when Linux resends an IRQ that was dropped earlier\n",
    "\t * WAITING is used by probe to mark irqs that are being tested\n",
    "\t */\n",
    "\tstatus = desc->status & ~(IRQ_REPLAY | IRQ_WAITING);\n",
    "\tstatus |= IRQ_PENDING; /* we _want_ to handle it */\n",
    "\n",
    "\t/*\n",
    "\t * If the IRQ is disabled for whatever reason, we cannot\n",
    "\t * use the action we have.\n",
    "\t */\n",
    "\taction = NULL;\n",
    "\tif (likely(!(status & (IRQ_DISABLED | IRQ_INPROGRESS)))) {\n",
    "\t\taction = desc->action;\n",
    "\t\tstatus &= ~IRQ_PENDING; /* we commit to handling */\n",
    "\t\tstatus |= IRQ_INPROGRESS; /* we are handling it */\n",
    "\t}\n",
    "\tdesc->status = status;\n",
    "\n",
    "\t/*\n",
    "\t * If there is no IRQ handler or it was disabled, exit early.\n",
    "\t * Since we set PENDING, if another processor is handling\n",
    "\t * a different instance of this same irq, the other processor\n",
    "\t * will take care of it.\n",
    "\t */\n",
    "\tif (unlikely(!action))\n",
    "\t\tgoto out;\n",
    "\n",
    "\t/*\n",
    "\t * Edge triggered interrupts need to remember\n",
    "\t * pending events.\n",
    "\t * This applies to any hw interrupts that allow a second\n",
    "\t * instance of the same irq to arrive while we are in do_IRQ\n",
    "\t * or in the handler. But the code here only handles the _second_\n",
    "\t * instance of the irq, not the third or fourth. So it is mostly\n",
    "\t * useful for irq hardware that does not mask cleanly in an\n",
    "\t * SMP environment.\n",
    "\t */\n",
    "\tfor (;;) {\n",
    "\t\tirqreturn_t action_ret;\n",
    "\n",
    "\t\tspin_unlock(&desc->lock);\n",
    "\n",
    "\t\taction_ret = handle_IRQ_event(irq, regs, action);\n",
    "\n",
    "\t\tspin_lock(&desc->lock);\n",
    "\t\tif (!noirqdebug)\n",
    "\t\t\tnote_interrupt(irq, desc, action_ret);\n",
    "\t\tif (likely(!(desc->status & IRQ_PENDING)))\n",
    "\t\t\tbreak;\n",
    "\t\tdesc->status &= ~IRQ_PENDING;\n",
    "\t}\n",
    "\tdesc->status &= ~IRQ_INPROGRESS;\n",
    "\n",
    "out:\n",
    "\t/*\n",
    "\t * The ->end() handler has to deal with interrupts which got\n",
    "\t * disabled while the handler was running.\n",
    "\t */\n",
    "\tdesc->handler->end(irq);\n",
    "\tspin_unlock(&desc->lock);\n",
    "\n",
    "\treturn 1;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "1. comments in the code\n",
    "\n",
    "2. ![](resources/int17.png)\n",
    "\n",
    "------------------------------------------\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linux2.6/kernel/irq/manage.c\n",
    "\n",
    "### irq cpu affinity\n",
    "\n",
    "```c\n",
    "cpumask_t irq_affinity[NR_IRQS] = { [0 ... NR_IRQS-1] = CPU_MASK_ALL };\n",
    "```\n",
    "\n",
    "### synchronize_irq\n",
    "\n",
    "```c\n",
    "/**\n",
    " *\tsynchronize_irq - wait for pending IRQ handlers (on other CPUs)\n",
    " *\n",
    " *\tThis function waits for any pending IRQ handlers for this interrupt\n",
    " *\tto complete before returning. If you use this function while\n",
    " *\tholding a resource the IRQ handler may need you will deadlock.\n",
    " *\n",
    " *\tThis function may be called - with care - from IRQ context.\n",
    " */\n",
    "void synchronize_irq(unsigned int irq)\n",
    "{\n",
    "\tstruct irq_desc *desc = irq_desc + irq;\n",
    "\n",
    "\twhile (desc->status & IRQ_INPROGRESS)\n",
    "\t\tcpu_relax();\n",
    "}\n",
    "```\n",
    "\n",
    "1. cpu_relax\n",
    "\n",
    "```c\n",
    "/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */\n",
    "static __always_inline void rep_nop(void)\n",
    "{\n",
    "\tasm volatile(\"rep; nop\" ::: \"memory\");\n",
    "}\n",
    "\n",
    "static __always_inline void cpu_relax(void)\n",
    "{\n",
    "\trep_nop();\n",
    "}\n",
    "```\n",
    "\n",
    "2. this function is used to wait for the handler ending.\n",
    "\n",
    "### enable/disable irq\n",
    "\n",
    "```c\n",
    "/**\n",
    " *\tdisable_irq_nosync - disable an irq without waiting\n",
    " *\t@irq: Interrupt to disable\n",
    " *\n",
    " *\tDisable the selected interrupt line.  Disables and Enables are\n",
    " *\tnested.\n",
    " *\tUnlike disable_irq(), this function does not ensure existing\n",
    " *\tinstances of the IRQ handler have completed before returning.\n",
    " *\n",
    " *\tThis function may be called from IRQ context.\n",
    " */\n",
    "void disable_irq_nosync(unsigned int irq)\n",
    "{\n",
    "\tirq_desc_t *desc = irq_desc + irq;\n",
    "\tunsigned long flags;\n",
    "\n",
    "\tspin_lock_irqsave(&desc->lock, flags);\n",
    "\tif (!desc->depth++) {\n",
    "\t\tdesc->status |= IRQ_DISABLED;\n",
    "\t\tdesc->handler->disable(irq);\n",
    "\t}\n",
    "\tspin_unlock_irqrestore(&desc->lock, flags);\n",
    "}\n",
    "\n",
    "EXPORT_SYMBOL(disable_irq_nosync);\n",
    "\n",
    "/**\n",
    " *\tdisable_irq - disable an irq and wait for completion\n",
    " *\t@irq: Interrupt to disable\n",
    " *\n",
    " *\tDisable the selected interrupt line.  Enables and Disables are\n",
    " *\tnested.\n",
    " *\tThis function waits for any pending IRQ handlers for this interrupt\n",
    " *\tto complete before returning. If you use this function while\n",
    " *\tholding a resource the IRQ handler may need you will deadlock.\n",
    " *\n",
    " *\tThis function may be called - with care - from IRQ context.\n",
    " */\n",
    "void disable_irq(unsigned int irq)\n",
    "{\n",
    "\tirq_desc_t *desc = irq_desc + irq;\n",
    "\n",
    "\tdisable_irq_nosync(irq);\n",
    "\tif (desc->action)\n",
    "\t\tsynchronize_irq(irq);\n",
    "}\n",
    "\n",
    "EXPORT_SYMBOL(disable_irq);\n",
    "\n",
    "/**\n",
    " *\tenable_irq - enable handling of an irq\n",
    " *\t@irq: Interrupt to enable\n",
    " *\n",
    " *\tUndoes the effect of one call to disable_irq().  If this\n",
    " *\tmatches the last disable, processing of interrupts on this\n",
    " *\tIRQ line is re-enabled.\n",
    " *\n",
    " *\tThis function may be called from IRQ context.\n",
    " */\n",
    "void enable_irq(unsigned int irq)\n",
    "{\n",
    "\tirq_desc_t *desc = irq_desc + irq;\n",
    "\tunsigned long flags;\n",
    "\n",
    "\tspin_lock_irqsave(&desc->lock, flags);\n",
    "\tswitch (desc->depth) {\n",
    "\tcase 0:\n",
    "\t\tWARN_ON(1);\n",
    "\t\tbreak;\n",
    "\tcase 1: {\n",
    "\t\tunsigned int status = desc->status & ~IRQ_DISABLED;\n",
    "\n",
    "\t\tdesc->status = status;\n",
    "\t\tif ((status & (IRQ_PENDING | IRQ_REPLAY)) == IRQ_PENDING) {\n",
    "\t\t\tdesc->status = status | IRQ_REPLAY;\n",
    "\t\t\thw_resend_irq(desc->handler,irq);\n",
    "\t\t}\n",
    "\t\tdesc->handler->enable(irq);\n",
    "\t\t/* fall-through */\n",
    "\t}\n",
    "\tdefault:\n",
    "\t\tdesc->depth--;\n",
    "\t}\n",
    "\tspin_unlock_irqrestore(&desc->lock, flags);\n",
    "}\n",
    "\n",
    "EXPORT_SYMBOL(enable_irq);\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Dynamic allocation of IRQ lines\n",
    "\n",
    "#### setup_irq\n",
    "\n",
    "```c\n",
    "/*\n",
    " * Internal function to register an irqaction - typically used to\n",
    " * allocate special interrupts that are part of the architecture.\n",
    " */\n",
    "int setup_irq(unsigned int irq, struct irqaction * new)\n",
    "{\n",
    "\tstruct irq_desc *desc = irq_desc + irq;\n",
    "\tstruct irqaction *old, **p;\n",
    "\tunsigned long flags;\n",
    "\tint shared = 0;\n",
    "\n",
    "\tif (desc->handler == &no_irq_type)\n",
    "\t\treturn -ENOSYS;\n",
    "\t/*\n",
    "\t * Some drivers like serial.c use request_irq() heavily,\n",
    "\t * so we have to be careful not to interfere with a\n",
    "\t * running system.\n",
    "\t */\n",
    "\tif (new->flags & SA_SAMPLE_RANDOM) {\n",
    "\t\t/*\n",
    "\t\t * This function might sleep, we want to call it first,\n",
    "\t\t * outside of the atomic block.\n",
    "\t\t * Yes, this might clear the entropy pool if the wrong\n",
    "\t\t * driver is attempted to be loaded, without actually\n",
    "\t\t * installing a new handler, but is this really a problem,\n",
    "\t\t * only the sysadmin is able to do this.\n",
    "\t\t */\n",
    "\t\trand_initialize_irq(irq);\n",
    "\t}\n",
    "\n",
    "\t/*\n",
    "\t * The following block of code has to be executed atomically\n",
    "\t */\n",
    "\tspin_lock_irqsave(&desc->lock,flags);\n",
    "\tp = &desc->action;\n",
    "\tif ((old = *p) != NULL) {\n",
    "\t\t/* Can't share interrupts unless both agree to */\n",
    "\t\tif (!(old->flags & new->flags & SA_SHIRQ)) {\n",
    "\t\t\tspin_unlock_irqrestore(&desc->lock,flags);\n",
    "\t\t\treturn -EBUSY;\n",
    "\t\t}\n",
    "\n",
    "\t\t/* add new interrupt at end of irq queue */\n",
    "\t\tdo {\n",
    "\t\t\tp = &old->next;\n",
    "\t\t\told = *p;\n",
    "\t\t} while (old);\n",
    "\t\tshared = 1;\n",
    "\t}\n",
    "\n",
    "\t*p = new;\n",
    "\n",
    "\tif (!shared) {\n",
    "\t\tdesc->depth = 0;\n",
    "\t\tdesc->status &= ~(IRQ_DISABLED | IRQ_AUTODETECT |\n",
    "\t\t\t\t  IRQ_WAITING | IRQ_INPROGRESS);\n",
    "\t\tif (desc->handler->startup)\n",
    "\t\t\tdesc->handler->startup(irq);\n",
    "\t\telse\n",
    "\t\t\tdesc->handler->enable(irq);\n",
    "\t}\n",
    "\tspin_unlock_irqrestore(&desc->lock,flags);\n",
    "\n",
    "\tnew->irq = irq;\n",
    "\tregister_irq_proc(irq);\n",
    "\tnew->dir = NULL;\n",
    "\tregister_handler_proc(irq, new);\n",
    "\n",
    "\treturn 0;\n",
    "}\n",
    "```\n",
    "\n",
    "#### request_irq\n",
    "\n",
    "```c\n",
    "\n",
    "/**\n",
    " *\trequest_irq - allocate an interrupt line\n",
    " *\t@irq: Interrupt line to allocate\n",
    " *\t@handler: Function to be called when the IRQ occurs\n",
    " *\t@irqflags: Interrupt type flags\n",
    " *\t@devname: An ascii name for the claiming device\n",
    " *\t@dev_id: A cookie passed back to the handler function\n",
    " *\n",
    " *\tThis call allocates interrupt resources and enables the\n",
    " *\tinterrupt line and IRQ handling. From the point this\n",
    " *\tcall is made your handler function may be invoked. Since\n",
    " *\tyour handler function must clear any interrupt the board\n",
    " *\traises, you must take care both to initialise your hardware\n",
    " *\tand to set up the interrupt handler in the right order.\n",
    " *\n",
    " *\tDev_id must be globally unique. Normally the address of the\n",
    " *\tdevice data structure is used as the cookie. Since the handler\n",
    " *\treceives this value it makes sense to use it.\n",
    " *\n",
    " *\tIf your interrupt is shared you must pass a non NULL dev_id\n",
    " *\tas this is required when freeing the interrupt.\n",
    " *\n",
    " *\tFlags:\n",
    " *\n",
    " *\tSA_SHIRQ\t\tInterrupt is shared\n",
    " *\tSA_INTERRUPT\t\tDisable local interrupts while processing\n",
    " *\tSA_SAMPLE_RANDOM\tThe interrupt can be used for entropy\n",
    " *\n",
    " */\n",
    "int request_irq(unsigned int irq,\n",
    "\t\tirqreturn_t (*handler)(int, void *, struct pt_regs *),\n",
    "\t\tunsigned long irqflags, const char * devname, void *dev_id)\n",
    "{\n",
    "\tstruct irqaction * action;\n",
    "\tint retval;\n",
    "\n",
    "\t/*\n",
    "\t * Sanity-check: shared interrupts must pass in a real dev-ID,\n",
    "\t * otherwise we'll have trouble later trying to figure out\n",
    "\t * which interrupt is which (messes up the interrupt freeing\n",
    "\t * logic etc).\n",
    "\t */\n",
    "\tif ((irqflags & SA_SHIRQ) && !dev_id)\n",
    "\t\treturn -EINVAL;\n",
    "\tif (irq >= NR_IRQS)\n",
    "\t\treturn -EINVAL;\n",
    "\tif (!handler)\n",
    "\t\treturn -EINVAL;\n",
    "\n",
    "\taction = kmalloc(sizeof(struct irqaction), GFP_ATOMIC);\n",
    "\tif (!action)\n",
    "\t\treturn -ENOMEM;\n",
    "\n",
    "\taction->handler = handler;\n",
    "\taction->flags = irqflags;\n",
    "\tcpus_clear(action->mask);\n",
    "\taction->name = devname;\n",
    "\taction->next = NULL;\n",
    "\taction->dev_id = dev_id;\n",
    "\n",
    "\tretval = setup_irq(irq, action);\n",
    "\tif (retval)\n",
    "\t\tkfree(action);\n",
    "\n",
    "\treturn retval;\n",
    "}\n",
    "\n",
    "EXPORT_SYMBOL(request_irq);\n",
    "```\n",
    "\n",
    "1. ![](resources/int18.png)\n",
    "\n",
    "2. code is straight\n",
    "\n",
    "--------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softirq & tasklet\n",
    "\n",
    "![](resources/softiqandtasklet01.png)\n",
    "![](resources/softiqandtasklet02.png)\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softirq\n",
    "\n",
    "![](resources/softirq01.png)\n",
    "![](resources/softirq02.png)\n",
    "\n",
    "\n",
    "### linux2.6/kernel/softirq.c\n",
    "\n",
    "#### global status variables\n",
    "\n",
    "```c\n",
    "/*\n",
    "   - No shared variables, all the data are CPU local.\n",
    "   - If a softirq needs serialization, let it serialize itself\n",
    "     by its own spinlocks.\n",
    "   - Even if softirq is serialized, only local cpu is marked for\n",
    "     execution. Hence, we get something sort of weak cpu binding.\n",
    "     Though it is still not clear, will it result in better locality\n",
    "     or will not.\n",
    "\n",
    "   Examples:\n",
    "   - NET RX softirq. It is multithreaded and does not require\n",
    "     any global serialization.\n",
    "   - NET TX softirq. It kicks software netdevice queues, hence\n",
    "     it is logically serialized per device, but this serialization\n",
    "     is invisible to common code.\n",
    "   - Tasklets: serialized wrt itself.\n",
    " */\n",
    "\n",
    "#ifndef __ARCH_IRQ_STAT\n",
    "irq_cpustat_t irq_stat[NR_CPUS] ____cacheline_aligned;\n",
    "EXPORT_SYMBOL(irq_stat);\n",
    "#endif\n",
    "\n",
    "static struct softirq_action softirq_vec[32] __cacheline_aligned_in_smp;\n",
    "\n",
    "static DEFINE_PER_CPU(struct task_struct *, ksoftirqd);\n",
    "```\n",
    "\n",
    "1. `irq_cpustat_t` stores every CPU irq info. It's an array for every cpu.\n",
    "\n",
    "```c\n",
    "typedef struct {\n",
    "\tunsigned int __softirq_pending;\n",
    "\tunsigned long idle_timestamp;\n",
    "\tunsigned int __nmi_count;\t/* arch dependent */\n",
    "\tunsigned int apic_timer_irqs;\t/* arch dependent */\n",
    "} ____cacheline_aligned irq_cpustat_t;\n",
    "```\n",
    "\n",
    "2. `softirq_action` \n",
    "\n",
    "```c\n",
    "/* softirq mask and active fields moved to irq_cpustat_t in\n",
    " * asm/hardirq.h to get better cache usage.  KAO\n",
    " */\n",
    "\n",
    "struct softirq_action\n",
    "{\n",
    "\tvoid\t(*action)(struct softirq_action *);\n",
    "\tvoid\t*data;\n",
    "};\n",
    "```\n",
    "\n",
    "3. `ksoftirqd` is the kernel routines to run the softirq actions.\n",
    "\n",
    "![](resources/softirq03.png)\n",
    "![](resources/softirq04.png)\n",
    "\n",
    "-----------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linux2.6/kernel/softirq.c\n",
    "\n",
    "```c\n",
    "asmlinkage void do_softirq(void)\n",
    "{\n",
    "\t__u32 pending;\n",
    "\tunsigned long flags;\n",
    "\n",
    "\tif (in_interrupt())\n",
    "\t\treturn;\n",
    "\n",
    "\tlocal_irq_save(flags);\n",
    "\n",
    "\tpending = local_softirq_pending();\n",
    "\n",
    "\tif (pending)\n",
    "\t\t__do_softirq();\n",
    "\n",
    "\tlocal_irq_restore(flags);\n",
    "}\n",
    "\n",
    "EXPORT_SYMBOL(do_softirq);\n",
    "```\n",
    "\n",
    "1. `in_interrupt`\n",
    "\n",
    "### linux2.6/include/linux/hardirq.h\n",
    "\n",
    "```c\n",
    "#define irq_count()\t(preempt_count() & (HARDIRQ_MASK | SOFTIRQ_MASK))\n",
    "#define in_interrupt()\t\t(irq_count())\n",
    "```\n",
    "\n",
    "### linux2.6/include/linux/preempt.h\n",
    "\n",
    "```c\n",
    "#define preempt_count()\t(current_thread_info()->preempt_count)\n",
    "```\n",
    "\n",
    "`preempt_count` is a field in the thread_info.\n",
    "\n",
    "![](resources/preempt_count01.png)\n",
    "![](resources/preempt_count02.png)\n",
    "![](resources/preempt_count03.png)\n",
    "\n",
    "![](resources/preempt01.png)\n",
    "![](resources/preempt02.png)\n",
    "![](resources/preempt03.png)\n",
    "\n",
    "![](resources/softirq05.png)\n",
    "\n",
    "\n",
    "\n",
    "-------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do_softirq\n",
    "\n",
    "```c\n",
    "/*\n",
    " * We restart softirq processing MAX_SOFTIRQ_RESTART times,\n",
    " * and we fall back to softirqd after that.\n",
    " *\n",
    " * This number has been established via experimentation.\n",
    " * The two things to balance is latency against fairness -\n",
    " * we want to handle softirqs as soon as possible, but they\n",
    " * should not be able to lock up the box.\n",
    " */\n",
    "#define MAX_SOFTIRQ_RESTART 10\n",
    "\n",
    "asmlinkage void __do_softirq(void)\n",
    "{\n",
    "\tstruct softirq_action *h;\n",
    "\t__u32 pending;\n",
    "\tint max_restart = MAX_SOFTIRQ_RESTART;\n",
    "\tint cpu;\n",
    "\n",
    "\tpending = local_softirq_pending();\n",
    "\n",
    "\tlocal_bh_disable();\n",
    "\tcpu = smp_processor_id();\n",
    "restart:\n",
    "\t/* Reset the pending bitmask before enabling irqs */\n",
    "\tlocal_softirq_pending() = 0;\n",
    "\n",
    "\tlocal_irq_enable();\n",
    "\n",
    "\th = softirq_vec;\n",
    "\n",
    "\tdo {\n",
    "\t\tif (pending & 1) {\n",
    "\t\t\th->action(h);\n",
    "\t\t\trcu_bh_qsctr_inc(cpu);\n",
    "\t\t}\n",
    "\t\th++;\n",
    "\t\tpending >>= 1;\n",
    "\t} while (pending);\n",
    "\n",
    "\tlocal_irq_disable();\n",
    "\n",
    "\tpending = local_softirq_pending();\n",
    "\tif (pending && --max_restart)\n",
    "\t\tgoto restart;\n",
    "\n",
    "\tif (pending)\n",
    "\t\twakeup_softirqd();\n",
    "\n",
    "\t__local_bh_enable();\n",
    "}\n",
    "\n",
    "#ifndef __ARCH_HAS_DO_SOFTIRQ\n",
    "\n",
    "asmlinkage void do_softirq(void)\n",
    "{\n",
    "\t__u32 pending;\n",
    "\tunsigned long flags;\n",
    "\n",
    "\tif (in_interrupt())\n",
    "\t\treturn;\n",
    "\n",
    "\tlocal_irq_save(flags);\n",
    "\n",
    "\tpending = local_softirq_pending();\n",
    "\n",
    "\tif (pending)\n",
    "\t\t__do_softirq();\n",
    "\n",
    "\tlocal_irq_restore(flags);\n",
    "}\n",
    "\n",
    "EXPORT_SYMBOL(do_softirq);\n",
    "```\n",
    "\n",
    "![](resources/do_softirq01.png)\n",
    "![](resources/do_softirq02.png)\n",
    "\n",
    "------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ksoftirqd\n",
    "\n",
    "### linux2.6/kernel/softifrq.c\n",
    "\n",
    "\n",
    "```c\n",
    "static int ksoftirqd(void * __bind_cpu)\n",
    "{\n",
    "\tset_user_nice(current, 19);\n",
    "\tcurrent->flags |= PF_NOFREEZE;\n",
    "\n",
    "\tset_current_state(TASK_INTERRUPTIBLE);\n",
    "\n",
    "\twhile (!kthread_should_stop()) {\n",
    "\t\tif (!local_softirq_pending())\n",
    "\t\t\tschedule();\n",
    "\n",
    "\t\t__set_current_state(TASK_RUNNING);\n",
    "\n",
    "\t\twhile (local_softirq_pending()) {\n",
    "\t\t\t/* Preempt disable stops cpu going offline.\n",
    "\t\t\t   If already offline, we'll be on wrong CPU:\n",
    "\t\t\t   don't process */\n",
    "\t\t\tpreempt_disable();\n",
    "\t\t\tif (cpu_is_offline((long)__bind_cpu))\n",
    "\t\t\t\tgoto wait_to_die;\n",
    "\t\t\tdo_softirq();\n",
    "\t\t\tpreempt_enable();\n",
    "\t\t\tcond_resched();\n",
    "\t\t}\n",
    "\n",
    "\t\tset_current_state(TASK_INTERRUPTIBLE);\n",
    "\t}\n",
    "\t__set_current_state(TASK_RUNNING);\n",
    "\treturn 0;\n",
    "\n",
    "wait_to_die:\n",
    "\tpreempt_enable();\n",
    "\t/* Wait for kthread_stop */\n",
    "\tset_current_state(TASK_INTERRUPTIBLE);\n",
    "\twhile (!kthread_should_stop()) {\n",
    "\t\tschedule();\n",
    "\t\tset_current_state(TASK_INTERRUPTIBLE);\n",
    "\t}\n",
    "\t__set_current_state(TASK_RUNNING);\n",
    "\treturn 0;\n",
    "}\n",
    "```\n",
    "\n",
    "1. this is the function ksoftirqd \n",
    "\n",
    "-------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## work queues\n",
    "\n",
    "![](resources/workqueue01.png)\n",
    "\n",
    "\n",
    "### linux2.6/kernel/workqueue.c\n",
    "```c\n",
    "\n",
    "/*\n",
    " * The per-CPU workqueue (if single thread, we always use cpu 0's).\n",
    " *\n",
    " * The sequence counters are for flush_scheduled_work().  It wants to wait\n",
    " * until until all currently-scheduled works are completed, but it doesn't\n",
    " * want to be livelocked by new, incoming ones.  So it waits until\n",
    " * remove_sequence is >= the insert_sequence which pertained when\n",
    " * flush_scheduled_work() was called.\n",
    " */\n",
    "struct cpu_workqueue_struct {\n",
    "\n",
    "\tspinlock_t lock;\n",
    "\n",
    "\tlong remove_sequence;\t/* Least-recently added (next to run) */\n",
    "\tlong insert_sequence;\t/* Next to add */\n",
    "\n",
    "\tstruct list_head worklist;\n",
    "\twait_queue_head_t more_work;\n",
    "\twait_queue_head_t work_done;\n",
    "\n",
    "\tstruct workqueue_struct *wq;\n",
    "\ttask_t *thread;\n",
    "\n",
    "\tint run_depth;\t\t/* Detect run_workqueue() recursion depth */\n",
    "} ____cacheline_aligned;\n",
    "\n",
    "/*\n",
    " * The externally visible workqueue abstraction is an array of\n",
    " * per-CPU workqueues:\n",
    " */\n",
    "struct workqueue_struct {\n",
    "\tstruct cpu_workqueue_struct cpu_wq[NR_CPUS];\n",
    "\tconst char *name;\n",
    "\tstruct list_head list; \t/* Empty if single thread */\n",
    "};\n",
    "\n",
    "/* All the per-cpu workqueues on the system, for hotplug cpu to add/remove\n",
    "   threads to each one as cpus come/go. */\n",
    "static DEFINE_SPINLOCK(workqueue_lock);\n",
    "static LIST_HEAD(workqueues);\n",
    "```\n",
    "\n",
    "1. per-cpu has its own queue\n",
    "\n",
    "2. `more-work` is the wait queue to store the workqueue kthread \n",
    "\n",
    "3. `work_done` is the wait queue to store the other threads to wait the workqueue task done.\n",
    "\n",
    "\n",
    "```c\n",
    "static int worker_thread(void *__cwq)\n",
    "{\n",
    "\tstruct cpu_workqueue_struct *cwq = __cwq;\n",
    "\tDECLARE_WAITQUEUE(wait, current);\n",
    "\tstruct k_sigaction sa;\n",
    "\tsigset_t blocked;\n",
    "\n",
    "\tcurrent->flags |= PF_NOFREEZE;\n",
    "\n",
    "\tset_user_nice(current, -5);\n",
    "\n",
    "\t/* Block and flush all signals */\n",
    "\tsigfillset(&blocked);\n",
    "\tsigprocmask(SIG_BLOCK, &blocked, NULL);\n",
    "\tflush_signals(current);\n",
    "\n",
    "\t/* SIG_IGN makes children autoreap: see do_notify_parent(). */\n",
    "\tsa.sa.sa_handler = SIG_IGN;\n",
    "\tsa.sa.sa_flags = 0;\n",
    "\tsiginitset(&sa.sa.sa_mask, sigmask(SIGCHLD));\n",
    "\tdo_sigaction(SIGCHLD, &sa, (struct k_sigaction *)0);\n",
    "\n",
    "\tset_current_state(TASK_INTERRUPTIBLE);\n",
    "\twhile (!kthread_should_stop()) {\n",
    "\t\tadd_wait_queue(&cwq->more_work, &wait);\n",
    "\t\tif (list_empty(&cwq->worklist))\n",
    "\t\t\tschedule();\n",
    "\t\telse\n",
    "\t\t\t__set_current_state(TASK_RUNNING);\n",
    "\t\tremove_wait_queue(&cwq->more_work, &wait);\n",
    "\n",
    "\t\tif (!list_empty(&cwq->worklist))\n",
    "\t\t\trun_workqueue(cwq);\n",
    "\t\tset_current_state(TASK_INTERRUPTIBLE);\n",
    "\t}\n",
    "\t__set_current_state(TASK_RUNNING);\n",
    "\treturn 0;\n",
    "}\n",
    "```\n",
    "\n",
    "1. This is the workqueue kthread. `wait` is current, which is the workqueue kthread. Add current to the wait queue `more_work`.\n",
    "   \n",
    "2. If the worklist is empty, `schedule` out. Or remove self from the wait queue and run the worklist.\n",
    "\n",
    "3. When it is scheduled out, the other threads queue some new task to worklist will wake up the workqueue kthread.\n",
    "\n",
    "\n",
    "```c\n",
    "/* Preempt must be disabled. */\n",
    "static void __queue_work(struct cpu_workqueue_struct *cwq,\n",
    "\t\t\t struct work_struct *work)\n",
    "{\n",
    "\tunsigned long flags;\n",
    "\n",
    "\tspin_lock_irqsave(&cwq->lock, flags);\n",
    "\twork->wq_data = cwq;\n",
    "\tlist_add_tail(&work->entry, &cwq->worklist);\n",
    "\tcwq->insert_sequence++;\n",
    "\twake_up(&cwq->more_work);\n",
    "\tspin_unlock_irqrestore(&cwq->lock, flags);\n",
    "}\n",
    "\n",
    "/*\n",
    " * Queue work on a workqueue. Return non-zero if it was successfully\n",
    " * added.\n",
    " *\n",
    " * We queue the work to the CPU it was submitted, but there is no\n",
    " * guarantee that it will be processed by that CPU.\n",
    " */\n",
    "int fastcall queue_work(struct workqueue_struct *wq, struct work_struct *work)\n",
    "{\n",
    "\tint ret = 0, cpu = get_cpu();\n",
    "\n",
    "\tif (!test_and_set_bit(0, &work->pending)) {\n",
    "\t\tif (unlikely(is_single_threaded(wq)))\n",
    "\t\t\tcpu = 0;\n",
    "\t\tBUG_ON(!list_empty(&work->entry));\n",
    "\t\t__queue_work(wq->cpu_wq + cpu, work);\n",
    "\t\tret = 1;\n",
    "\t}\n",
    "\tput_cpu();\n",
    "\treturn ret;\n",
    "}\n",
    "```\n",
    "\n",
    "1. `__queue_work` will wake up the waitqueue kthread to do the works.\n",
    "\n",
    "\n",
    "```c\n",
    "static void flush_cpu_workqueue(struct cpu_workqueue_struct *cwq)\n",
    "{\n",
    "\tif (cwq->thread == current) {\n",
    "\t\t/*\n",
    "\t\t * Probably keventd trying to flush its own queue. So simply run\n",
    "\t\t * it by hand rather than deadlocking.\n",
    "\t\t */\n",
    "\t\trun_workqueue(cwq);\n",
    "\t} else {\n",
    "\t\tDEFINE_WAIT(wait);\n",
    "\t\tlong sequence_needed;\n",
    "\n",
    "\t\tspin_lock_irq(&cwq->lock);\n",
    "\t\tsequence_needed = cwq->insert_sequence;\n",
    "\n",
    "\t\twhile (sequence_needed - cwq->remove_sequence > 0) {\n",
    "\t\t\tprepare_to_wait(&cwq->work_done, &wait,\n",
    "\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n",
    "\t\t\tspin_unlock_irq(&cwq->lock);\n",
    "\t\t\tschedule();\n",
    "\t\t\tspin_lock_irq(&cwq->lock);\n",
    "\t\t}\n",
    "\t\tfinish_wait(&cwq->work_done, &wait);\n",
    "\t\tspin_unlock_irq(&cwq->lock);\n",
    "\t}\n",
    "}\n",
    "\n",
    "/*\n",
    " * flush_workqueue - ensure that any scheduled work has run to completion.\n",
    " *\n",
    " * Forces execution of the workqueue and blocks until its completion.\n",
    " * This is typically used in driver shutdown handlers.\n",
    " *\n",
    " * This function will sample each workqueue's current insert_sequence number and\n",
    " * will sleep until the head sequence is greater than or equal to that.  This\n",
    " * means that we sleep until all works which were queued on entry have been\n",
    " * handled, but we are not livelocked by new incoming ones.\n",
    " *\n",
    " * This function used to run the workqueues itself.  Now we just wait for the\n",
    " * helper threads to do it.\n",
    " */\n",
    "void fastcall flush_workqueue(struct workqueue_struct *wq)\n",
    "{\n",
    "\tmight_sleep();\n",
    "\n",
    "\tif (is_single_threaded(wq)) {\n",
    "\t\t/* Always use cpu 0's area. */\n",
    "\t\tflush_cpu_workqueue(wq->cpu_wq + 0);\n",
    "\t} else {\n",
    "\t\tint cpu;\n",
    "\n",
    "\t\tlock_cpu_hotplug();\n",
    "\t\tfor_each_online_cpu(cpu)\n",
    "\t\t\tflush_cpu_workqueue(wq->cpu_wq + cpu);\n",
    "\t\tunlock_cpu_hotplug();\n",
    "\t}\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "1. If the waitqueue kthread call `flush_cpu_workqueue`, just do the tasks. Or add `current` to the `work_done` wait queue and schedule out. It will be waked up in the `run_workqueue`\n",
    "\n",
    "2. `sequence_needed - cwq->remove_sequence > 0` Flush means flush all current work list. `sequence_needed` is the old value when it call flush and `cwq->remove_sequence` is the live value, which is changed during `run_workqueue`. When all the tasks are done, it will stop waitting. Or it will schedule out and wait again.\n",
    "\n",
    "\n",
    "```c\n",
    "static inline void run_workqueue(struct cpu_workqueue_struct *cwq)\n",
    "{\n",
    "\tunsigned long flags;\n",
    "\n",
    "\t/*\n",
    "\t * Keep taking off work from the queue until\n",
    "\t * done.\n",
    "\t */\n",
    "\tspin_lock_irqsave(&cwq->lock, flags);\n",
    "\tcwq->run_depth++;\n",
    "\tif (cwq->run_depth > 3) {\n",
    "\t\t/* morton gets to eat his hat */\n",
    "\t\tprintk(\"%s: recursion depth exceeded: %d\\n\",\n",
    "\t\t\t__FUNCTION__, cwq->run_depth);\n",
    "\t\tdump_stack();\n",
    "\t}\n",
    "\twhile (!list_empty(&cwq->worklist)) {\n",
    "\t\tstruct work_struct *work = list_entry(cwq->worklist.next,\n",
    "\t\t\t\t\t\tstruct work_struct, entry);\n",
    "\t\tvoid (*f) (void *) = work->func;\n",
    "\t\tvoid *data = work->data;\n",
    "\n",
    "\t\tlist_del_init(cwq->worklist.next);\n",
    "\t\tspin_unlock_irqrestore(&cwq->lock, flags);\n",
    "\n",
    "\t\tBUG_ON(work->wq_data != cwq);\n",
    "\t\tclear_bit(0, &work->pending);\n",
    "\t\tf(data);\n",
    "\n",
    "\t\tspin_lock_irqsave(&cwq->lock, flags);\n",
    "\t\tcwq->remove_sequence++;\n",
    "\t\twake_up(&cwq->work_done);\n",
    "\t}\n",
    "\tcwq->run_depth--;\n",
    "\tspin_unlock_irqrestore(&cwq->lock, flags);\n",
    "}\n",
    "```\n",
    "\n",
    "1. Run the worklist and wake up the `work_done` list. See previous description.\n",
    "\n",
    "2. workqueue use the `work_more` and `word_done` wait queues to sync the workqueue kthread and other threads.\n",
    "\n",
    "\n",
    "```c\n",
    "int fastcall queue_delayed_work(struct workqueue_struct *wq,\n",
    "\t\t\tstruct work_struct *work, unsigned long delay)\n",
    "{\n",
    "\tint ret = 0;\n",
    "\tstruct timer_list *timer = &work->timer;\n",
    "\n",
    "\tif (!test_and_set_bit(0, &work->pending)) {\n",
    "\t\tBUG_ON(timer_pending(timer));\n",
    "\t\tBUG_ON(!list_empty(&work->entry));\n",
    "\n",
    "\t\t/* This stores wq for the moment, for the timer_fn */\n",
    "\t\twork->wq_data = wq;\n",
    "\t\ttimer->expires = jiffies + delay;\n",
    "\t\ttimer->data = (unsigned long)work;\n",
    "\t\ttimer->function = delayed_work_timer_fn;\n",
    "\t\tadd_timer(timer);\n",
    "\t\tret = 1;\n",
    "\t}\n",
    "\treturn ret;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "1. delayed work is just to add a timer to add work.\n",
    "   \n",
    "-----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1be8c8bdc6fd86eebde1498154837c0fcc29bda10079a062db8bdccc6e4e2b17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
